{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing_Phase.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEYSaXlFJUrD"
      },
      "source": [
        "\n",
        "`**Copyright of Gajarajan, 2021 **       `\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9a5C07S1jK6"
      },
      "source": [
        "# *Running VSCCPSO with Euclidean distance*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8UW7jcNWOyJ",
        "outputId": "b4563113-f267-45c4-a349-214e1d391268"
      },
      "source": [
        "!git clone https://github.com/jundongl/scikit-feature.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'scikit-feature'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 1086 (delta 54), reused 105 (delta 33), pack-reused 957\u001b[K\n",
            "Receiving objects: 100% (1086/1086), 194.80 MiB | 36.72 MiB/s, done.\n",
            "Resolving deltas: 100% (640/640), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYMFxtJqfgU7",
        "outputId": "b552964f-cd51-4660-c5bd-bda8d9038611"
      },
      "source": [
        "cd scikit-feature/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/scikit-feature\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_JM0nOWf-wK",
        "outputId": "385c7849-f9bb-4b69-afce-2fd274bd841b"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/skfeature\n",
            "copying skfeature/__init__.py -> build/lib/skfeature\n",
            "creating build/lib/skfeature/utility\n",
            "copying skfeature/utility/mutual_information.py -> build/lib/skfeature/utility\n",
            "copying skfeature/utility/unsupervised_evaluation.py -> build/lib/skfeature/utility\n",
            "copying skfeature/utility/entropy_estimators.py -> build/lib/skfeature/utility\n",
            "copying skfeature/utility/sparse_learning.py -> build/lib/skfeature/utility\n",
            "copying skfeature/utility/data_discretization.py -> build/lib/skfeature/utility\n",
            "copying skfeature/utility/construct_W.py -> build/lib/skfeature/utility\n",
            "copying skfeature/utility/__init__.py -> build/lib/skfeature/utility\n",
            "creating build/lib/skfeature/function\n",
            "copying skfeature/function/__init__.py -> build/lib/skfeature/function\n",
            "creating build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/MRMR.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/CMIM.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/MIFS.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/DISR.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/FCBF.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/LCSI.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/ICAP.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/MIM.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/CIFE.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/__init__.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "copying skfeature/function/information_theoretical_based/JMI.py -> build/lib/skfeature/function/information_theoretical_based\n",
            "creating build/lib/skfeature/function/similarity_based\n",
            "copying skfeature/function/similarity_based/SPEC.py -> build/lib/skfeature/function/similarity_based\n",
            "copying skfeature/function/similarity_based/fisher_score.py -> build/lib/skfeature/function/similarity_based\n",
            "copying skfeature/function/similarity_based/lap_score.py -> build/lib/skfeature/function/similarity_based\n",
            "copying skfeature/function/similarity_based/trace_ratio.py -> build/lib/skfeature/function/similarity_based\n",
            "copying skfeature/function/similarity_based/__init__.py -> build/lib/skfeature/function/similarity_based\n",
            "copying skfeature/function/similarity_based/reliefF.py -> build/lib/skfeature/function/similarity_based\n",
            "creating build/lib/skfeature/function/sparse_learning_based\n",
            "copying skfeature/function/sparse_learning_based/ls_l21.py -> build/lib/skfeature/function/sparse_learning_based\n",
            "copying skfeature/function/sparse_learning_based/NDFS.py -> build/lib/skfeature/function/sparse_learning_based\n",
            "copying skfeature/function/sparse_learning_based/ll_l21.py -> build/lib/skfeature/function/sparse_learning_based\n",
            "copying skfeature/function/sparse_learning_based/MCFS.py -> build/lib/skfeature/function/sparse_learning_based\n",
            "copying skfeature/function/sparse_learning_based/RFS.py -> build/lib/skfeature/function/sparse_learning_based\n",
            "copying skfeature/function/sparse_learning_based/UDFS.py -> build/lib/skfeature/function/sparse_learning_based\n",
            "copying skfeature/function/sparse_learning_based/__init__.py -> build/lib/skfeature/function/sparse_learning_based\n",
            "creating build/lib/skfeature/function/statistical_based\n",
            "copying skfeature/function/statistical_based/f_score.py -> build/lib/skfeature/function/statistical_based\n",
            "copying skfeature/function/statistical_based/gini_index.py -> build/lib/skfeature/function/statistical_based\n",
            "copying skfeature/function/statistical_based/chi_square.py -> build/lib/skfeature/function/statistical_based\n",
            "copying skfeature/function/statistical_based/CFS.py -> build/lib/skfeature/function/statistical_based\n",
            "copying skfeature/function/statistical_based/t_score.py -> build/lib/skfeature/function/statistical_based\n",
            "copying skfeature/function/statistical_based/low_variance.py -> build/lib/skfeature/function/statistical_based\n",
            "copying skfeature/function/statistical_based/__init__.py -> build/lib/skfeature/function/statistical_based\n",
            "creating build/lib/skfeature/function/streaming\n",
            "copying skfeature/function/streaming/alpha_investing.py -> build/lib/skfeature/function/streaming\n",
            "copying skfeature/function/streaming/__init__.py -> build/lib/skfeature/function/streaming\n",
            "creating build/lib/skfeature/function/structure\n",
            "copying skfeature/function/structure/graph_fs.py -> build/lib/skfeature/function/structure\n",
            "copying skfeature/function/structure/tree_fs.py -> build/lib/skfeature/function/structure\n",
            "copying skfeature/function/structure/group_fs.py -> build/lib/skfeature/function/structure\n",
            "copying skfeature/function/structure/__init__.py -> build/lib/skfeature/function/structure\n",
            "creating build/lib/skfeature/function/wrapper\n",
            "copying skfeature/function/wrapper/svm_forward.py -> build/lib/skfeature/function/wrapper\n",
            "copying skfeature/function/wrapper/decision_tree_backward.py -> build/lib/skfeature/function/wrapper\n",
            "copying skfeature/function/wrapper/svm_backward.py -> build/lib/skfeature/function/wrapper\n",
            "copying skfeature/function/wrapper/__init__.py -> build/lib/skfeature/function/wrapper\n",
            "copying skfeature/function/wrapper/decision_tree_forward.py -> build/lib/skfeature/function/wrapper\n",
            "running install_lib\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function/streaming\n",
            "copying build/lib/skfeature/function/streaming/alpha_investing.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/streaming\n",
            "copying build/lib/skfeature/function/streaming/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/streaming\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "copying build/lib/skfeature/function/statistical_based/f_score.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "copying build/lib/skfeature/function/statistical_based/gini_index.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "copying build/lib/skfeature/function/statistical_based/chi_square.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "copying build/lib/skfeature/function/statistical_based/CFS.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "copying build/lib/skfeature/function/statistical_based/t_score.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "copying build/lib/skfeature/function/statistical_based/low_variance.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "copying build/lib/skfeature/function/statistical_based/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based\n",
            "copying build/lib/skfeature/function/similarity_based/SPEC.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based\n",
            "copying build/lib/skfeature/function/similarity_based/fisher_score.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based\n",
            "copying build/lib/skfeature/function/similarity_based/lap_score.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based\n",
            "copying build/lib/skfeature/function/similarity_based/trace_ratio.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based\n",
            "copying build/lib/skfeature/function/similarity_based/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based\n",
            "copying build/lib/skfeature/function/similarity_based/reliefF.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper\n",
            "copying build/lib/skfeature/function/wrapper/svm_forward.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper\n",
            "copying build/lib/skfeature/function/wrapper/decision_tree_backward.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper\n",
            "copying build/lib/skfeature/function/wrapper/svm_backward.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper\n",
            "copying build/lib/skfeature/function/wrapper/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper\n",
            "copying build/lib/skfeature/function/wrapper/decision_tree_forward.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "copying build/lib/skfeature/function/sparse_learning_based/ls_l21.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "copying build/lib/skfeature/function/sparse_learning_based/NDFS.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "copying build/lib/skfeature/function/sparse_learning_based/ll_l21.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "copying build/lib/skfeature/function/sparse_learning_based/MCFS.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "copying build/lib/skfeature/function/sparse_learning_based/RFS.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "copying build/lib/skfeature/function/sparse_learning_based/UDFS.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "copying build/lib/skfeature/function/sparse_learning_based/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function/structure\n",
            "copying build/lib/skfeature/function/structure/graph_fs.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/structure\n",
            "copying build/lib/skfeature/function/structure/tree_fs.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/structure\n",
            "copying build/lib/skfeature/function/structure/group_fs.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/structure\n",
            "copying build/lib/skfeature/function/structure/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/structure\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/MRMR.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/CMIM.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/MIFS.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/DISR.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/FCBF.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/LCSI.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/ICAP.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/MIM.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/CIFE.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/information_theoretical_based/JMI.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based\n",
            "copying build/lib/skfeature/function/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/function\n",
            "creating /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/utility/mutual_information.py -> /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/utility/unsupervised_evaluation.py -> /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/utility/entropy_estimators.py -> /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/utility/sparse_learning.py -> /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/utility/data_discretization.py -> /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/utility/construct_W.py -> /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/utility/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature/utility\n",
            "copying build/lib/skfeature/__init__.py -> /usr/local/lib/python3.6/dist-packages/skfeature\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/streaming/alpha_investing.py to alpha_investing.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/streaming/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based/f_score.py to f_score.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based/gini_index.py to gini_index.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based/chi_square.py to chi_square.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based/CFS.py to CFS.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based/t_score.py to t_score.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based/low_variance.py to low_variance.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/statistical_based/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based/SPEC.py to SPEC.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based/fisher_score.py to fisher_score.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based/lap_score.py to lap_score.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based/trace_ratio.py to trace_ratio.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/similarity_based/reliefF.py to reliefF.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper/svm_forward.py to svm_forward.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper/decision_tree_backward.py to decision_tree_backward.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper/svm_backward.py to svm_backward.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/wrapper/decision_tree_forward.py to decision_tree_forward.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based/ls_l21.py to ls_l21.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based/NDFS.py to NDFS.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based/ll_l21.py to ll_l21.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based/MCFS.py to MCFS.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based/RFS.py to RFS.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based/UDFS.py to UDFS.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/sparse_learning_based/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/structure/graph_fs.py to graph_fs.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/structure/tree_fs.py to tree_fs.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/structure/group_fs.py to group_fs.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/structure/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/MRMR.py to MRMR.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/CMIM.py to CMIM.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/MIFS.py to MIFS.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/DISR.py to DISR.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/FCBF.py to FCBF.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/LCSI.py to LCSI.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/ICAP.py to ICAP.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/MIM.py to MIM.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/CIFE.py to CIFE.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/information_theoretical_based/JMI.py to JMI.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/function/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/utility/mutual_information.py to mutual_information.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/utility/unsupervised_evaluation.py to unsupervised_evaluation.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/utility/entropy_estimators.py to entropy_estimators.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/utility/sparse_learning.py to sparse_learning.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/utility/data_discretization.py to data_discretization.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/utility/construct_W.py to construct_W.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/utility/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/skfeature/__init__.py to __init__.cpython-36.pyc\n",
            "running install_egg_info\n",
            "Writing /usr/local/lib/python3.6/dist-packages/skfeature-1.0.0.egg-info\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef3aJv9wHaSF"
      },
      "source": [
        "# import traceback\n",
        "# import copy\n",
        "# M_arr=[1,2,3,4,5,6,8,10]\n",
        "# #M_arr=M_arr[::-1]\n",
        "# for M in M_arr:\n",
        "#   import pandas as pd\n",
        "#   import numpy as np\n",
        "#   from skfeature.utility.mutual_information import su_calculation\n",
        "#   import math\n",
        "#   import pickle\n",
        "#   from sklearn.impute import SimpleImputer\n",
        "#   from sklearn.preprocessing import LabelEncoder\n",
        "#   import os\n",
        "#   import time\n",
        "#   enc = LabelEncoder()\n",
        "#   imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "#   #ds_name='test_dataset_workflow'\n",
        "#   ds_name='SRBCT'\n",
        "#   df=pd.read_csv(('/content/drive/My Drive/FYP/Dataset/%s.csv')%(ds_name))\n",
        "#   df=df.replace('?',np.nan)\n",
        "#   enc.fit(df[df.columns[-1]])\n",
        "#   df[df.columns[-1]] = enc.transform(df[df.columns[-1]])\n",
        "#   imp=imp.fit(df)\n",
        "#   data_arr=imp.transform(df)\n",
        "#   #print(df.columns)\n",
        "#   df= pd.DataFrame(data=data_arr,columns=df.columns)\n",
        "#   print(df.head())\n",
        "#   ## creating directories\n",
        "#   if not os.path.exists(('/content/drive/My Drive/FYP/%s')%(ds_name)):\n",
        "#       reduced_datasets=('/content/drive/My Drive/FYP/%s/reduced_datasets')%(ds_name)\n",
        "#       results_csv=(('/content/drive/My Drive/FYP/%s/results_csv')%(ds_name))\n",
        "#       temp_storage=('/content/drive/My Drive/FYP/%s/temp')%(ds_name)\n",
        "#       results_graphs=(('/content/drive/My Drive/FYP/%s/results_graphs')%(ds_name))\n",
        "#       os.makedirs(('/content/drive/My Drive/FYP/%s')%(ds_name))\n",
        "#       os.makedirs(reduced_datasets)\n",
        "#       os.makedirs(results_csv)\n",
        "#       os.makedirs(results_graphs)\n",
        "#       os.makedirs(temp_storage)\n",
        "    \n",
        "#   else:\n",
        "    \n",
        "#     print(\"Error Already File Exists!!!!\")\n",
        "#     reduced_datasets=('/content/drive/My Drive/FYP/%s/reduced_datasets')%(ds_name)\n",
        "#     results_csv=(('/content/drive/My Drive/FYP/%s/results_csv')%(ds_name))\n",
        "#     temp_storage=('/content/drive/My Drive/FYP/%s/temp')%(ds_name)\n",
        "#     results_graphs=(('/content/drive/My Drive/FYP/%s/results_graphs')%(ds_name))\n",
        "#     import sys\n",
        "#     #sys.exit(\"Error message\")\n",
        "#   start = time.time()\n",
        "#   ##only one gbest\n",
        "#   gbest_tracer=-1\n",
        "#   ### Removing irrelvant features with delta value\n",
        "#   X = df.values\n",
        "#   n_samples, n_features = X.shape\n",
        "#   n_features = n_features-1\n",
        "#   y = X[:,-1]\n",
        "#   # t1[:,0] stores index of features, t1[:,1] stores symmetrical uncertainty of features\n",
        "#   t1 = np.zeros((n_features, 2))\n",
        "#   for i in range(n_features):\n",
        "#       f = X[:, i]\n",
        "#       t1[i, 0] = i\n",
        "#       t1[i, 1] = su_calculation(f, y)\n",
        "#   su_max = t1[:,1].max()\n",
        "#   delta = 0.1 * su_max\n",
        "#   '''print(\"Symmetrical Uncertainity \",t1[:,1])\n",
        "#   print(\"MAX SU \",su_max)\n",
        "#   '''\n",
        "#   F = t1[t1[:, 1] > delta, :]\n",
        "#   '''print(\"Features Selected \",F)'''\n",
        "#   print((\"No. of Features selected with threshold :%f\")%(len(F)))\n",
        "#   ### Calculating sub-swarm sizes and feature in sub-swarms\n",
        "#   # # sub swarms\n",
        "#   #M=10\n",
        "\n",
        "#   n_s_features=F.shape[0]\n",
        "#   #temp_fe=n_s_features\n",
        "#   temp_fe=n_s_features\n",
        "#   # if temp_fe<500:\n",
        "#   #   M=3\n",
        "#   # elif temp_fe>1000 and temp_fe<4000:\n",
        "#   #   M=6\n",
        "#   # elif temp_fe>4000:\n",
        "#   #   M=10\n",
        "#   # else:\n",
        "#   #   M=5\n",
        "#   #M=1\n",
        "#   print(\"No. of Sub-swarms \",M)\n",
        "#   reduced_datasets+='/M_'+str(M)\n",
        "#   results_csv+='/M_'+str(M)\n",
        "#   temp_storage+='/M_'+str(M)\n",
        "#   results_graphs+='/M_'+str(M)\n",
        "#   os.makedirs(reduced_datasets)\n",
        "#   os.makedirs(results_csv)\n",
        "#   os.makedirs(results_graphs)\n",
        "#   os.makedirs(temp_storage)\n",
        "#   #M=10\n",
        "#   N=n_s_features//20\n",
        "#   N=min(N,300)\n",
        "#   # if N<=0:\n",
        "#   #   N=4*n_s_features\n",
        "#   #   M=2\n",
        "#   #   print(\"No. of sub-swarms changed to \",M)\n",
        "#   N=max(N,100)\n",
        "#   l=math.floor(n_s_features/M)\n",
        "#   F1=F[F[:,1].argsort(kind='mergesort')]\n",
        "#   F1=F1[::-1]\n",
        "#   '''print(\"Descending order by SU \",F1)'''\n",
        "#   U= []\n",
        "#   s=0\n",
        "#   for i in range(M):\n",
        "#     if s+l-1 < n_s_features:\n",
        "#       #U.append(F1[s:s+l-1,0])\n",
        "#       U.append(F1[s:s+l,0])\n",
        "#       s=s+l\n",
        "#     else:\n",
        "#       #U.append(F1[s:n_s_features-1,0])\n",
        "#       U.append(F1[s:n_s_features,0])\n",
        "#   '''print(\"Index of original features in Sub-swarms \",U)'''\n",
        "\n",
        "#   ### initialization of sub-swarms\n",
        "#   Fim=[]\n",
        "#   for i in U:\n",
        "#     #print(i)\n",
        "#     sum=0\n",
        "#     for j in i:\n",
        "#       #print(j)\n",
        "#       j=int(j)\n",
        "#       f=X[:,j]\n",
        "#       sum = sum+su_calculation(f, y)\n",
        "#     Fim.append(sum)\n",
        "\n",
        "#   '''print(\"Feature Importances \",Fim)'''\n",
        "#   SN=[]\n",
        "#   sum_fim=np.sum(Fim)\n",
        "#   #N=200\n",
        "#   #N=len(F)//20\n",
        "\n",
        "#   #N=math.floor((N+300)/2)\n",
        "#   #N=math.floor(0.5*(N+300))\n",
        "#   # if N <300:\n",
        "#   #   N=300\n",
        "#   # if N <300:\n",
        "#   #   N=300\n",
        "#   #N=300\n",
        "#   #N=300\n",
        "#   print(\"No. of particles \",N)\n",
        "#   for i in Fim:\n",
        "#     SN.append(math.floor(i/sum_fim*N))\n",
        "#   SN_max=min(N,2*N/M)\n",
        "#   SN_min=min(5,N/(2*M))\n",
        "#   for j,i in enumerate(SN):\n",
        "#     if i > SN_max:\n",
        "#       SN[j]=int(SN_max)\n",
        "#     elif i >= SN_min and i <= SN_max:\n",
        "#       SN[j]=int(i)\n",
        "#     elif i < SN_min:\n",
        "#       SN[j]=int(SN_min)\n",
        "#   print(\"Population size in sub-swarms \",SN)\n",
        "#   ### Main mechanisms\n",
        "#   import warnings\n",
        "#   warnings.filterwarnings('ignore')\n",
        "#   import random\n",
        "#   #from datetime import datetime\n",
        "\n",
        "#   #print time\n",
        "\n",
        "#   time_cal=0\n",
        "#   # now = datetime.now()\n",
        "#   # current_time = now.strftime(\"%H:%M:%S\")\n",
        "#   # print(\"Current Time =\", current_time)\n",
        "#   #inertia set\n",
        "#   #w=0.7298\n",
        "#   #c1=1.49618\n",
        "#   #c2=1.49618\n",
        "#   max_eval=7000\n",
        "#   curr_eval=0\n",
        "#   #max_eval=100\n",
        "#   swarms={}\n",
        "#   #created commonly for easy of calculations\n",
        "#   swarms['sub-swarm common rel_con']=np.zeros(len(SN))\n",
        "#   swarms['sub-swarm common rel_div']=np.zeros(len(SN))\n",
        "#   #adding filter\n",
        "#   # global_best_tracer=0.0\n",
        "#   # g_b_t_ind=0\n",
        "#   print(\"Initializing........\")\n",
        "#   for i in range(0,len(SN)):\n",
        "#     swarms['sub-swarm '+str(i)]={}\n",
        "#     swarms['sub-swarm '+str(i)]['gbest']=None\n",
        "#     swarms['sub-swarm '+str(i)]['gbest-val']=None\n",
        "#     swarms['sub-swarm '+str(i)]['particles']={}\n",
        "#     ##added for divergence\n",
        "#     swarms['sub-swarm '+str(i)]['fit-par-t']=np.zeros(SN[i])\n",
        "#     swarms['sub-swarm '+str(i)]['fit-par-t-1']=np.zeros(SN[i])\n",
        "#     ###\n",
        "#     sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "#     for j in range(0,SN[i]):\n",
        "#       sub_swarm_particles['particle '+str(j)]={}\n",
        "#       sub_swarm_particles['particle '+str(j)]['cur_pos']=np.random.uniform(0,1,len(U[i]))\n",
        "#       #sub_swarm_particles['particle '+str(j)]['cur_vel']=np.random.uniform(0,1,len(U[i]))\n",
        "#       sub_swarm_particles['particle '+str(j)]['best_pos']=copy.deepcopy(sub_swarm_particles['particle '+str(j)]['cur_pos'])\n",
        "#       d_t=sub_swarm_particles['particle '+str(j)]['best_pos-val']=fitness_particle(swarms,i,j,curr_eval)\n",
        "#       #print(d_t)\n",
        "#       '''print(\"current,best position,fitness of particle of particle \",i,\" \",j,\" \",sub_swarm_particles['particle '+str(j)]['cur_pos'],sub_swarm_particles['particle '+str(j)]['best_pos'],d_t)\n",
        "#       '''\n",
        "#       #if (swarms['sub-swarm '+str(i)]['gbest'] is None )or (d_t>swarms['sub-swarm '+str(i)]['gbest-val']):\n",
        "#       if (d_t>gbest_tracer):\n",
        "#         '''print(\"Global best position,fitness before change \",swarms['sub-swarm '+str(i)]['gbest'],swarms['sub-swarm '+str(i)]['gbest-val'])\n",
        "#         '''\n",
        "#         swarms['sub-swarm '+str(i)]['gbest']=copy.deepcopy(sub_swarm_particles['particle '+str(j)]['cur_pos'])\n",
        "#         swarms['sub-swarm '+str(i)]['gbest-val']=d_t\n",
        "#         gbest_tracer=d_t\n",
        "#         '''print(\"Global best position,fitness after change \",swarms['sub-swarm '+str(i)]['gbest'],swarms['sub-swarm '+str(i)]['gbest-val'])\n",
        "#         '''\n",
        "#         #print(d_t)\n",
        "#         #sub_swarm_particles['particle '+str(j)]['best_pos-val']=***********\n",
        "#       else:\n",
        "#         if swarms['sub-swarm '+str(i)]['gbest-val']!=gbest_tracer:\n",
        "#           swarms['sub-swarm '+str(i)]['gbest']=np.zeros(len(U[i]))\n",
        "#           swarms['sub-swarm '+str(i)]['gbest-val']=gbest_tracer\n",
        "#       temp_p_f=d_t\n",
        "#       swarms['sub-swarm '+str(i)]['fit-par-t'][j]=temp_p_f\n",
        "#       swarms['sub-swarm '+str(i)]['fit-par-t-1'][j]=temp_p_f\n",
        "#       '''print(\"Fitness of particle in t,t-1 iterations \",swarms['sub-swarm '+str(i)]['fit-par-t'][j])\n",
        "#       print(swarms['sub-swarm '+str(i)]['fit-par-t-1'][j])\n",
        "#       '''\n",
        "    \n",
        "#     ##adding fbest for sub-swarm\n",
        "\n",
        "#     temp_g_f=swarms['sub-swarm '+str(i)]['gbest-val']\n",
        "#     # if global_best_tracer<temp_g_f:\n",
        "#     #   global_best_tracer=temp_g_f\n",
        "#     #   g_b_t_ind=int(i)\n",
        "#     print(\"Global best particle accuracy in sub-swarm \"+str(i)+\" is \",temp_g_f)\n",
        "#     swarms['sub-swarm '+str(i)]['fbest t']=temp_g_f\n",
        "#     swarms['sub-swarm '+str(i)]['fbest t-1']=temp_g_f\n",
        "#     swarms['sub-swarm '+str(i)]['fbest t-2']=temp_g_f\n",
        "#     '''print(\"Best fitness of particle in \",i,\" \",j,\" t,t-1,t-2 iterations\",swarms['sub-swarm '+str(i)]['fbest t'],swarms['sub-swarm '+str(i)]['fbest t-1'],swarms['sub-swarm '+str(i)]['fbest t-2'])\n",
        "#     '''\n",
        "#     print('Initializing Sub-Swarm '+str(i)+' Finished')\n",
        "#   #deleting unwanted sub-swarms\n",
        "#   # t_l=len(U[i])\n",
        "#   # for k in range(g_b_t_ind+1,t_l):\n",
        "#   #   del(swarms['sub-swarm '+str(k)])\n",
        "#   #   del(SN[k])\n",
        "#   #   del(U[k])\n",
        "#   #   del(Fim[k])\n",
        "#   # sum_fim=np.sum(Fim)\n",
        "#   c=1\n",
        "#   #stop criteria\n",
        "#   #stop_cre=100\n",
        "#   stop_cre=30\n",
        "#   #freq=0\n",
        "#   while c<=stop_cre :\n",
        "  \n",
        "#     SN=[int(ele) for ele in SN]\n",
        "#     #w=0.9-0.5*c/stop_cre #Good Results when w is not dynamic\n",
        "#     print(\"Iteration # \"+str(c))\n",
        "#     print(\"Population size in sub-swarms \",SN)\n",
        "#     #print(\"Value of w :\",str(w))\n",
        "#     pbest_cal=0\n",
        "#     for n_i in range(0,len(SN)):\n",
        "#       i=swarms['sub-swarm '+str(n_i)]\n",
        "#       if gbest_tracer!=swarms['sub-swarm '+str(n_i)]['gbest-val']:\n",
        "#         #swarms['sub-swarm '+str(n_i)]['gbest-val']=fitness_gbest(swarms,n_i,curr_eval)\n",
        "#         swarms['sub-swarm '+str(n_i)]['gbest-val']=gbest_tracer\n",
        "#         pbest_cal=1\n",
        "#       else:\n",
        "#         pbest_cal=0\n",
        "#       #print(i['particles'])\n",
        "#       for n_j in range(0,SN[n_i]):\n",
        "#         '''print(\"For \",n_i,\" \",n_j)'''\n",
        "#         j=i['particles']['particle '+str(n_j)]\n",
        "#         if pbest_cal:\n",
        "#           swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']=fitness_particle_re(swarms,n_i,n_j,curr_eval)\n",
        "#           if  swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']>gbest_tracer:\n",
        "#             gbest_tracer= swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']\n",
        "#             swarms['sub-swarm '+str(n_i)]['gbest-val']=gbest_tracer\n",
        "#             swarms['sub-swarm '+str(n_i)]['gbest']=copy.deepcopy(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos'])\n",
        "#         #j['cur_vel']=w*j['cur_vel']+(c1*random.uniform(0, 1))*(j['best_pos']-j['cur_pos'])+(c2*random.uniform(0, 1))*(i['gbest']-j['cur_pos'])\n",
        "#         mu=0.5*(j['best_pos']+i['gbest'])\n",
        "#         pb1=i['particles']['particle '+str(np.random.randint(0,SN[n_i]))]['best_pos']\n",
        "#         pb2=i['particles']['particle '+str(np.random.randint(0,SN[n_i]))]['best_pos']\n",
        "#         #tri=np.random.uniform(0,1)*np.absolute(pb1-pb2)*math.exp(fitness_particle(swarms,n_i,n_j,curr_eval)-fitness_gbest(swarms,n_i,curr_eval))\n",
        "#         sub_swarm_particles=swarms['sub-swarm '+str(n_i)]['particles']\n",
        "#         tri=np.random.uniform(0,1)*np.absolute(pb1-pb2)*math.exp(sub_swarm_particles['particle '+str(n_j)]['best_pos-val']-swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "#         delta=np.absolute(j['best_pos']-i['gbest'])+tri\n",
        "#         r4=0.7\n",
        "#         cmpr4=np.random.uniform(0,1,len(j['cur_pos']))\n",
        "#         #print(len(j['cur_pos']))\n",
        "#         t_r4=cmpr4<r4\n",
        "#         f_r4=cmpr4>=r4\n",
        "#         '''\n",
        "#         print(\"Mu \",mu)\n",
        "#         print(\"tri \",tri)\n",
        "#         print(\"delta \",delta)\n",
        "#         print(\"random particle 1,2 \",pb1,pb2)\n",
        "#         print(np.random.normal(loc=mu,scale=delta))\n",
        "#         '''\n",
        "#         temp_store1=j['cur_pos']==1\n",
        "#         ### element-wise\n",
        "#         '''print(\"Before change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['cur_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])\n",
        "#         '''\n",
        "#         j['cur_pos'][t_r4]=np.random.normal(loc=mu,scale=delta,size=len(j['cur_pos']))[t_r4]\n",
        "#         j['cur_pos'][f_r4]=j['best_pos'][f_r4]\n",
        "#         # if np.random.uniform(0,1)<r4:\n",
        "#         #   j['cur_pos']=np.random.normal(loc=mu,scale=delta)\n",
        "#         # else:\n",
        "#         #   j['cur_pos']=j['best_pos']\n",
        "#         ###added 1 line\n",
        "#         #j['cur_vel']=normalizer(j['cur_vel'])\n",
        "#         #j['cur_pos']=j['cur_pos']+j['cur_vel']\n",
        "#         ###added 1 line\n",
        "#         #print(j['cur_pos'])\n",
        "#         j['cur_pos']=normalizer(j['cur_pos'])\n",
        "#         #######testing\n",
        "#         #print(id(j['cur_pos']))\n",
        "#         #print(id(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos']))\n",
        "#         swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos']=copy.deepcopy(j['cur_pos'])\n",
        "#         #print(j['cur_pos'])\n",
        "#         temp_store2=j['cur_pos']==1\n",
        "#         #print(temp_store2)\n",
        "#         #freq+=1\n",
        "#         cond=(temp_store1==temp_store2).all()\n",
        "        \n",
        "#         if not cond:\n",
        "#           #print(fitness_particle(swarms,i,j))\n",
        "#           #print(freq)\n",
        "#           try:\n",
        "#             d_t_p_c=fitness_particle(swarms,n_i,n_j,curr_eval)\n",
        "#             #print(id(d_t_p_c))\n",
        "#             d_t_p_b=sub_swarm_particles['particle '+str(n_j)]['best_pos-val']\n",
        "#             #d_t_p_b=fitness_particle_re(swarms,n_i,n_j,curr_eval)\n",
        "#             #print(d_t_p_c,d_t_p_b)\n",
        "#             '''print(\"After change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['cur_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])\n",
        "#             '''\n",
        "#             print(d_t_p_c,sub_swarm_particles['particle '+str(n_j)]['best_pos-val'],swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "#             if d_t_p_c>d_t_p_b:\n",
        "#               '''print(\"before change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])            \n",
        "#               '''\n",
        "#               #############testing\n",
        "#               swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos']=copy.deepcopy(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos'])\n",
        "#               swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']=d_t_p_c\n",
        "#               #print(id(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']))\n",
        "#               #print('Best position of paricle '+str(n_j)+'in sub-swarm '+str(n_i)+'changed\\n')\n",
        "#               d_t_g_b=swarms['sub-swarm '+str(n_i)]['gbest-val']\n",
        "#               #d_t_g_b=fitness_gbest(swarms,n_i,curr_eval)\n",
        "#               '''print(\"before change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])            \n",
        "#               '''\n",
        "#               #print(d_t_g_b)\n",
        "#               #print(d_t_p_c,d_t_g_b)\n",
        "#               if d_t_p_c>gbest_tracer:\n",
        "#                 '''print(\"Global best position,fitness before change \",swarms['sub-swarm '+str(n_i)]['gbest'],swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "#                 '''\n",
        "#                 #######testing\n",
        "#                 swarms['sub-swarm '+str(n_i)]  ['gbest']=copy.deepcopy(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos'])\n",
        "#                 swarms['sub-swarm '+str(n_i)]  ['gbest-val']=d_t_p_c\n",
        "#                 gbest_tracer=d_t_p_c\n",
        "#                 '''print(\"Global best position,fitness after change \",swarms['sub-swarm '+str(n_i)]['gbest'],swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "#                 '''\n",
        "#                 print('Global Best position in sub-swarm '+str(n_i)+'changed\\n')\n",
        "              \n",
        "#           except Exception:\n",
        "#             traceback.print_exc()\n",
        "#             print(\"e1\")\n",
        "#             pass\n",
        "#           try:\n",
        "#             #print(id(d_t_p_c))\n",
        "#             temp_p_f=d_t_p_c\n",
        "#             swarms['sub-swarm '+str(n_i)]['fit-par-t-1'][n_j]=swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]\n",
        "#             swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]=temp_p_f\n",
        "#             '''print(\"Fitness of particle in t,t-1 iterations \",swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j],swarms['sub-swarm '+str(n_i)]['fit-par-t-1'][n_j])\n",
        "#             '''\n",
        "#           except:\n",
        "#             #print(swarms['sub-swarm '+str(n_i)]['fit-par-t-1'])\n",
        "#             #print(n_i,n_j,SN[n_i])\n",
        "#             print(\"e2\")\n",
        "#             pass\n",
        "#       else:\n",
        "#         #print(\"Eureka!!\")\n",
        "#         try:\n",
        "#           swarms['sub-swarm '+str(n_i)]['fit-par-t-1'][n_j]=swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]\n",
        "#           swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]= swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]\n",
        "#         except:\n",
        "#           print(\"err\")\n",
        "#           pass\n",
        "#         curr_eval=curr_eval+1\n",
        "#         pass\n",
        "#       swarms['sub-swarm common rel_con'][n_i]=relative_convergence(swarms,n_i,i,curr_eval)\n",
        "#       swarms['sub-swarm common rel_div'][n_i]=relative_divergence(swarms,n_i)\n",
        "#     print(\"Relative Convergence:\")\n",
        "#     print(swarms['sub-swarm common rel_con'])\n",
        "#     print(\"Relative Divergence:\")\n",
        "#     print(swarms['sub-swarm common rel_div'])\n",
        "#       ##\n",
        "#       #print(i['gbest'])\n",
        "#     adaptive_sub_swarm_size(SN,swarms,N,M,max_eval,curr_eval)\n",
        "#     #if c%5==0:\n",
        "#     if True:\n",
        "#       done=time.time()\n",
        "#       elapsed=done-start\n",
        "#       elapsed+=time_cal\n",
        "#       time_cal=elapsed\n",
        "#       print((\"Total time taken: %f\")%(time_cal/60))\n",
        "#       rt=time_cal/60\n",
        "#       save_file(ds_name)\n",
        "#       start=time.time()\n",
        "#     c=c+1\n",
        "#   plot_pdf('naive bayes')\n",
        "#   plot_pdf('tree')\n",
        "#   plot_pdf('svm_linear')\n",
        "#   plot_pdf('knn')\n",
        "\n",
        "#   print(\"For Naive Bayes\")\n",
        "#   mean_clf('naive bayes')\n",
        "#   print(\"\\nFor Tree\")\n",
        "#   mean_clf('tree')\n",
        "#   print(\"\\nFor SVM Linear\")\n",
        "#   mean_clf('svm_linear')\n",
        "#   print(\"\\nFor KNN\")\n",
        "#   mean_clf('knn')\n",
        "#   # done=time.time()\n",
        "#   # elapsed=done-start-(extra_time)\n",
        "#   #print((\"Total time taken: %f\")%(elapsed/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTptSYURWgGa"
      },
      "source": [
        "from contextlib import redirect_stdout\n",
        "import datetime\n",
        "ds_name='Yale'\n",
        "with open(('/content/drive/My Drive/FYP/output_screen/%s_%s.txt')%(ds_name,datetime.datetime.now()), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "     import traceback\n",
        "      import copy\n",
        "      M_arr=[1,2,3,4,5,6,8,10]\n",
        "      #M_arr=M_arr[::-1]\n",
        "      for M in M_arr:\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        from skfeature.utility.mutual_information import su_calculation\n",
        "        import math\n",
        "        import pickle\n",
        "        from sklearn.impute import SimpleImputer\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        import os\n",
        "        import time\n",
        "        enc = LabelEncoder()\n",
        "        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "        #ds_name='test_dataset_workflow'\n",
        "        #ds_name='SRBCT'\n",
        "        df=pd.read_csv(('/content/drive/My Drive/FYP/Dataset/%s.csv')%(ds_name))\n",
        "        df=df.replace('?',np.nan)\n",
        "        enc.fit(df[df.columns[-1]])\n",
        "        df[df.columns[-1]] = enc.transform(df[df.columns[-1]])\n",
        "        imp=imp.fit(df)\n",
        "        data_arr=imp.transform(df)\n",
        "        #print(df.columns)\n",
        "        df= pd.DataFrame(data=data_arr,columns=df.columns)\n",
        "        print(df.head())\n",
        "        ## creating directories\n",
        "        if not os.path.exists(('/content/drive/My Drive/FYP/%s')%(ds_name)):\n",
        "            reduced_datasets=('/content/drive/My Drive/FYP/%s/reduced_datasets')%(ds_name)\n",
        "            results_csv=(('/content/drive/My Drive/FYP/%s/results_csv')%(ds_name))\n",
        "            temp_storage=('/content/drive/My Drive/FYP/%s/temp')%(ds_name)\n",
        "            results_graphs=(('/content/drive/My Drive/FYP/%s/results_graphs')%(ds_name))\n",
        "            os.makedirs(('/content/drive/My Drive/FYP/%s')%(ds_name))\n",
        "            os.makedirs(reduced_datasets)\n",
        "            os.makedirs(results_csv)\n",
        "            os.makedirs(results_graphs)\n",
        "            os.makedirs(temp_storage)\n",
        "          \n",
        "        else:\n",
        "          \n",
        "          print(\"Error Already File Exists!!!!\")\n",
        "          reduced_datasets=('/content/drive/My Drive/FYP/%s/reduced_datasets')%(ds_name)\n",
        "          results_csv=(('/content/drive/My Drive/FYP/%s/results_csv')%(ds_name))\n",
        "          temp_storage=('/content/drive/My Drive/FYP/%s/temp')%(ds_name)\n",
        "          results_graphs=(('/content/drive/My Drive/FYP/%s/results_graphs')%(ds_name))\n",
        "          import sys\n",
        "          #sys.exit(\"Error message\")\n",
        "        start = time.time()\n",
        "        ##only one gbest\n",
        "        gbest_tracer=-1\n",
        "        ### Removing irrelvant features with delta value\n",
        "        X = df.values\n",
        "        n_samples, n_features = X.shape\n",
        "        n_features = n_features-1\n",
        "        y = X[:,-1]\n",
        "        # t1[:,0] stores index of features, t1[:,1] stores symmetrical uncertainty of features\n",
        "        t1 = np.zeros((n_features, 2))\n",
        "        for i in range(n_features):\n",
        "            f = X[:, i]\n",
        "            t1[i, 0] = i\n",
        "            t1[i, 1] = su_calculation(f, y)\n",
        "        su_max = t1[:,1].max()\n",
        "        delta = 0.1 * su_max\n",
        "        '''print(\"Symmetrical Uncertainity \",t1[:,1])\n",
        "        print(\"MAX SU \",su_max)\n",
        "        '''\n",
        "        F = t1[t1[:, 1] > delta, :]\n",
        "        '''print(\"Features Selected \",F)'''\n",
        "        print((\"No. of Features selected with threshold :%f\")%(len(F)))\n",
        "        ### Calculating sub-swarm sizes and feature in sub-swarms\n",
        "        # # sub swarms\n",
        "        #M=10\n",
        "\n",
        "        n_s_features=F.shape[0]\n",
        "        #temp_fe=n_s_features\n",
        "        temp_fe=n_s_features\n",
        "        # if temp_fe<500:\n",
        "        #   M=3\n",
        "        # elif temp_fe>1000 and temp_fe<4000:\n",
        "        #   M=6\n",
        "        # elif temp_fe>4000:\n",
        "        #   M=10\n",
        "        # else:\n",
        "        #   M=5\n",
        "        #M=1\n",
        "        print(\"No. of Sub-swarms \",M)\n",
        "        reduced_datasets+='/M_'+str(M)\n",
        "        results_csv+='/M_'+str(M)\n",
        "        temp_storage+='/M_'+str(M)\n",
        "        results_graphs+='/M_'+str(M)\n",
        "        os.makedirs(reduced_datasets)\n",
        "        os.makedirs(results_csv)\n",
        "        os.makedirs(results_graphs)\n",
        "        os.makedirs(temp_storage)\n",
        "        #M=10\n",
        "        N=n_s_features//20\n",
        "        N=min(N,300)\n",
        "        # if N<=0:\n",
        "        #   N=4*n_s_features\n",
        "        #   M=2\n",
        "        #   print(\"No. of sub-swarms changed to \",M)\n",
        "        N=max(N,100)\n",
        "        l=math.floor(n_s_features/M)\n",
        "        F1=F[F[:,1].argsort(kind='mergesort')]\n",
        "        F1=F1[::-1]\n",
        "        '''print(\"Descending order by SU \",F1)'''\n",
        "        U= []\n",
        "        s=0\n",
        "        for i in range(M):\n",
        "          if s+l-1 < n_s_features:\n",
        "            #U.append(F1[s:s+l-1,0])\n",
        "            U.append(F1[s:s+l,0])\n",
        "            s=s+l\n",
        "          else:\n",
        "            #U.append(F1[s:n_s_features-1,0])\n",
        "            U.append(F1[s:n_s_features,0])\n",
        "        '''print(\"Index of original features in Sub-swarms \",U)'''\n",
        "\n",
        "        ### initialization of sub-swarms\n",
        "        Fim=[]\n",
        "        for i in U:\n",
        "          #print(i)\n",
        "          sum=0\n",
        "          for j in i:\n",
        "            #print(j)\n",
        "            j=int(j)\n",
        "            f=X[:,j]\n",
        "            sum = sum+su_calculation(f, y)\n",
        "          Fim.append(sum)\n",
        "\n",
        "        '''print(\"Feature Importances \",Fim)'''\n",
        "        SN=[]\n",
        "        sum_fim=np.sum(Fim)\n",
        "        #N=200\n",
        "        #N=len(F)//20\n",
        "\n",
        "        #N=math.floor((N+300)/2)\n",
        "        #N=math.floor(0.5*(N+300))\n",
        "        # if N <300:\n",
        "        #   N=300\n",
        "        # if N <300:\n",
        "        #   N=300\n",
        "        #N=300\n",
        "        #N=300\n",
        "        print(\"No. of particles \",N)\n",
        "        for i in Fim:\n",
        "          SN.append(math.floor(i/sum_fim*N))\n",
        "        SN_max=min(N,2*N/M)\n",
        "        SN_min=min(5,N/(2*M))\n",
        "        for j,i in enumerate(SN):\n",
        "          if i > SN_max:\n",
        "            SN[j]=int(SN_max)\n",
        "          elif i >= SN_min and i <= SN_max:\n",
        "            SN[j]=int(i)\n",
        "          elif i < SN_min:\n",
        "            SN[j]=int(SN_min)\n",
        "        print(\"Population size in sub-swarms \",SN)\n",
        "        ### Main mechanisms\n",
        "        import warnings\n",
        "        warnings.filterwarnings('ignore')\n",
        "        import random\n",
        "        #from datetime import datetime\n",
        "\n",
        "        #print time\n",
        "\n",
        "        time_cal=0\n",
        "        # now = datetime.now()\n",
        "        # current_time = now.strftime(\"%H:%M:%S\")\n",
        "        # print(\"Current Time =\", current_time)\n",
        "        #inertia set\n",
        "        #w=0.7298\n",
        "        #c1=1.49618\n",
        "        #c2=1.49618\n",
        "        max_eval=7000\n",
        "        curr_eval=0\n",
        "        #max_eval=100\n",
        "        swarms={}\n",
        "        #created commonly for easy of calculations\n",
        "        swarms['sub-swarm common rel_con']=np.zeros(len(SN))\n",
        "        swarms['sub-swarm common rel_div']=np.zeros(len(SN))\n",
        "        #adding filter\n",
        "        # global_best_tracer=0.0\n",
        "        # g_b_t_ind=0\n",
        "        print(\"Initializing........\")\n",
        "        for i in range(0,len(SN)):\n",
        "          swarms['sub-swarm '+str(i)]={}\n",
        "          swarms['sub-swarm '+str(i)]['gbest']=None\n",
        "          swarms['sub-swarm '+str(i)]['gbest-val']=None\n",
        "          swarms['sub-swarm '+str(i)]['particles']={}\n",
        "          ##added for divergence\n",
        "          swarms['sub-swarm '+str(i)]['fit-par-t']=np.zeros(SN[i])\n",
        "          swarms['sub-swarm '+str(i)]['fit-par-t-1']=np.zeros(SN[i])\n",
        "          ###\n",
        "          sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "          for j in range(0,SN[i]):\n",
        "            sub_swarm_particles['particle '+str(j)]={}\n",
        "            sub_swarm_particles['particle '+str(j)]['cur_pos']=np.random.uniform(0,1,len(U[i]))\n",
        "            #sub_swarm_particles['particle '+str(j)]['cur_vel']=np.random.uniform(0,1,len(U[i]))\n",
        "            sub_swarm_particles['particle '+str(j)]['best_pos']=copy.deepcopy(sub_swarm_particles['particle '+str(j)]['cur_pos'])\n",
        "            d_t=sub_swarm_particles['particle '+str(j)]['best_pos-val']=fitness_particle(swarms,i,j,curr_eval)\n",
        "            #print(d_t)\n",
        "            '''print(\"current,best position,fitness of particle of particle \",i,\" \",j,\" \",sub_swarm_particles['particle '+str(j)]['cur_pos'],sub_swarm_particles['particle '+str(j)]['best_pos'],d_t)\n",
        "            '''\n",
        "            #if (swarms['sub-swarm '+str(i)]['gbest'] is None )or (d_t>swarms['sub-swarm '+str(i)]['gbest-val']):\n",
        "            if (d_t>gbest_tracer):\n",
        "              '''print(\"Global best position,fitness before change \",swarms['sub-swarm '+str(i)]['gbest'],swarms['sub-swarm '+str(i)]['gbest-val'])\n",
        "              '''\n",
        "              swarms['sub-swarm '+str(i)]['gbest']=copy.deepcopy(sub_swarm_particles['particle '+str(j)]['cur_pos'])\n",
        "              swarms['sub-swarm '+str(i)]['gbest-val']=d_t\n",
        "              gbest_tracer=d_t\n",
        "              '''print(\"Global best position,fitness after change \",swarms['sub-swarm '+str(i)]['gbest'],swarms['sub-swarm '+str(i)]['gbest-val'])\n",
        "              '''\n",
        "              #print(d_t)\n",
        "              #sub_swarm_particles['particle '+str(j)]['best_pos-val']=***********\n",
        "            else:\n",
        "              if swarms['sub-swarm '+str(i)]['gbest-val']!=gbest_tracer:\n",
        "                swarms['sub-swarm '+str(i)]['gbest']=np.zeros(len(U[i]))\n",
        "                swarms['sub-swarm '+str(i)]['gbest-val']=gbest_tracer\n",
        "            temp_p_f=d_t\n",
        "            swarms['sub-swarm '+str(i)]['fit-par-t'][j]=temp_p_f\n",
        "            swarms['sub-swarm '+str(i)]['fit-par-t-1'][j]=temp_p_f\n",
        "            '''print(\"Fitness of particle in t,t-1 iterations \",swarms['sub-swarm '+str(i)]['fit-par-t'][j])\n",
        "            print(swarms['sub-swarm '+str(i)]['fit-par-t-1'][j])\n",
        "            '''\n",
        "          \n",
        "          ##adding fbest for sub-swarm\n",
        "\n",
        "          temp_g_f=swarms['sub-swarm '+str(i)]['gbest-val']\n",
        "          # if global_best_tracer<temp_g_f:\n",
        "          #   global_best_tracer=temp_g_f\n",
        "          #   g_b_t_ind=int(i)\n",
        "          print(\"Global best particle accuracy in sub-swarm \"+str(i)+\" is \",temp_g_f)\n",
        "          swarms['sub-swarm '+str(i)]['fbest t']=temp_g_f\n",
        "          swarms['sub-swarm '+str(i)]['fbest t-1']=temp_g_f\n",
        "          swarms['sub-swarm '+str(i)]['fbest t-2']=temp_g_f\n",
        "          '''print(\"Best fitness of particle in \",i,\" \",j,\" t,t-1,t-2 iterations\",swarms['sub-swarm '+str(i)]['fbest t'],swarms['sub-swarm '+str(i)]['fbest t-1'],swarms['sub-swarm '+str(i)]['fbest t-2'])\n",
        "          '''\n",
        "          print('Initializing Sub-Swarm '+str(i)+' Finished')\n",
        "        #deleting unwanted sub-swarms\n",
        "        # t_l=len(U[i])\n",
        "        # for k in range(g_b_t_ind+1,t_l):\n",
        "        #   del(swarms['sub-swarm '+str(k)])\n",
        "        #   del(SN[k])\n",
        "        #   del(U[k])\n",
        "        #   del(Fim[k])\n",
        "        # sum_fim=np.sum(Fim)\n",
        "        c=1\n",
        "        #stop criteria\n",
        "        #stop_cre=100\n",
        "        stop_cre=30\n",
        "        #freq=0\n",
        "        while c<=stop_cre :\n",
        "        \n",
        "          SN=[int(ele) for ele in SN]\n",
        "          #w=0.9-0.5*c/stop_cre #Good Results when w is not dynamic\n",
        "          print(\"Iteration # \"+str(c))\n",
        "          print(\"Population size in sub-swarms \",SN)\n",
        "          #print(\"Value of w :\",str(w))\n",
        "          pbest_cal=0\n",
        "          for n_i in range(0,len(SN)):\n",
        "            i=swarms['sub-swarm '+str(n_i)]\n",
        "            if gbest_tracer!=swarms['sub-swarm '+str(n_i)]['gbest-val']:\n",
        "              #swarms['sub-swarm '+str(n_i)]['gbest-val']=fitness_gbest(swarms,n_i,curr_eval)\n",
        "              swarms['sub-swarm '+str(n_i)]['gbest-val']=gbest_tracer\n",
        "              pbest_cal=1\n",
        "            else:\n",
        "              pbest_cal=0\n",
        "            #print(i['particles'])\n",
        "            for n_j in range(0,SN[n_i]):\n",
        "              '''print(\"For \",n_i,\" \",n_j)'''\n",
        "              j=i['particles']['particle '+str(n_j)]\n",
        "              if pbest_cal:\n",
        "                swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']=fitness_particle_re(swarms,n_i,n_j,curr_eval)\n",
        "                if  swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']>gbest_tracer:\n",
        "                  gbest_tracer= swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']\n",
        "                  swarms['sub-swarm '+str(n_i)]['gbest-val']=gbest_tracer\n",
        "                  swarms['sub-swarm '+str(n_i)]['gbest']=copy.deepcopy(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos'])\n",
        "              #j['cur_vel']=w*j['cur_vel']+(c1*random.uniform(0, 1))*(j['best_pos']-j['cur_pos'])+(c2*random.uniform(0, 1))*(i['gbest']-j['cur_pos'])\n",
        "              mu=0.5*(j['best_pos']+i['gbest'])\n",
        "              pb1=i['particles']['particle '+str(np.random.randint(0,SN[n_i]))]['best_pos']\n",
        "              pb2=i['particles']['particle '+str(np.random.randint(0,SN[n_i]))]['best_pos']\n",
        "              #tri=np.random.uniform(0,1)*np.absolute(pb1-pb2)*math.exp(fitness_particle(swarms,n_i,n_j,curr_eval)-fitness_gbest(swarms,n_i,curr_eval))\n",
        "              sub_swarm_particles=swarms['sub-swarm '+str(n_i)]['particles']\n",
        "              tri=np.random.uniform(0,1)*np.absolute(pb1-pb2)*math.exp(sub_swarm_particles['particle '+str(n_j)]['best_pos-val']-swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "              delta=np.absolute(j['best_pos']-i['gbest'])+tri\n",
        "              r4=0.7\n",
        "              cmpr4=np.random.uniform(0,1,len(j['cur_pos']))\n",
        "              #print(len(j['cur_pos']))\n",
        "              t_r4=cmpr4<r4\n",
        "              f_r4=cmpr4>=r4\n",
        "              '''\n",
        "              print(\"Mu \",mu)\n",
        "              print(\"tri \",tri)\n",
        "              print(\"delta \",delta)\n",
        "              print(\"random particle 1,2 \",pb1,pb2)\n",
        "              print(np.random.normal(loc=mu,scale=delta))\n",
        "              '''\n",
        "              temp_store1=j['cur_pos']==1\n",
        "              ### element-wise\n",
        "              '''print(\"Before change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['cur_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])\n",
        "              '''\n",
        "              j['cur_pos'][t_r4]=np.random.normal(loc=mu,scale=delta,size=len(j['cur_pos']))[t_r4]\n",
        "              j['cur_pos'][f_r4]=j['best_pos'][f_r4]\n",
        "              # if np.random.uniform(0,1)<r4:\n",
        "              #   j['cur_pos']=np.random.normal(loc=mu,scale=delta)\n",
        "              # else:\n",
        "              #   j['cur_pos']=j['best_pos']\n",
        "              ###added 1 line\n",
        "              #j['cur_vel']=normalizer(j['cur_vel'])\n",
        "              #j['cur_pos']=j['cur_pos']+j['cur_vel']\n",
        "              ###added 1 line\n",
        "              #print(j['cur_pos'])\n",
        "              j['cur_pos']=normalizer(j['cur_pos'])\n",
        "              #######testing\n",
        "              #print(id(j['cur_pos']))\n",
        "              #print(id(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos']))\n",
        "              swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos']=copy.deepcopy(j['cur_pos'])\n",
        "              #print(j['cur_pos'])\n",
        "              temp_store2=j['cur_pos']==1\n",
        "              #print(temp_store2)\n",
        "              #freq+=1\n",
        "              cond=(temp_store1==temp_store2).all()\n",
        "              \n",
        "              if not cond:\n",
        "                #print(fitness_particle(swarms,i,j))\n",
        "                #print(freq)\n",
        "                try:\n",
        "                  d_t_p_c=fitness_particle(swarms,n_i,n_j,curr_eval)\n",
        "                  #print(id(d_t_p_c))\n",
        "                  d_t_p_b=sub_swarm_particles['particle '+str(n_j)]['best_pos-val']\n",
        "                  #d_t_p_b=fitness_particle_re(swarms,n_i,n_j,curr_eval)\n",
        "                  #print(d_t_p_c,d_t_p_b)\n",
        "                  '''print(\"After change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['cur_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])\n",
        "                  '''\n",
        "                  print(d_t_p_c,sub_swarm_particles['particle '+str(n_j)]['best_pos-val'],swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "                  if d_t_p_c>d_t_p_b:\n",
        "                    '''print(\"before change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])            \n",
        "                    '''\n",
        "                    #############testing\n",
        "                    swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos']=copy.deepcopy(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos'])\n",
        "                    swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']=d_t_p_c\n",
        "                    #print(id(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['best_pos-val']))\n",
        "                    #print('Best position of paricle '+str(n_j)+'in sub-swarm '+str(n_i)+'changed\\n')\n",
        "                    d_t_g_b=swarms['sub-swarm '+str(n_i)]['gbest-val']\n",
        "                    #d_t_g_b=fitness_gbest(swarms,n_i,curr_eval)\n",
        "                    '''print(\"before change current,best position,fitness of particle of particle \",n_i,\" \",n_j,\" \",sub_swarm_particles['particle '+str(n_j)]['best_pos'],sub_swarm_particles['particle '+str(n_j)]['best_pos-val'])            \n",
        "                    '''\n",
        "                    #print(d_t_g_b)\n",
        "                    #print(d_t_p_c,d_t_g_b)\n",
        "                    if d_t_p_c>gbest_tracer:\n",
        "                      '''print(\"Global best position,fitness before change \",swarms['sub-swarm '+str(n_i)]['gbest'],swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "                      '''\n",
        "                      #######testing\n",
        "                      swarms['sub-swarm '+str(n_i)]  ['gbest']=copy.deepcopy(swarms['sub-swarm '+str(n_i)]['particles']['particle '+str(n_j)]['cur_pos'])\n",
        "                      swarms['sub-swarm '+str(n_i)]  ['gbest-val']=d_t_p_c\n",
        "                      gbest_tracer=d_t_p_c\n",
        "                      '''print(\"Global best position,fitness after change \",swarms['sub-swarm '+str(n_i)]['gbest'],swarms['sub-swarm '+str(n_i)]['gbest-val'])\n",
        "                      '''\n",
        "                      print('Global Best position in sub-swarm '+str(n_i)+'changed\\n')\n",
        "                    \n",
        "                except Exception:\n",
        "                  traceback.print_exc()\n",
        "                  print(\"e1\")\n",
        "                  pass\n",
        "                try:\n",
        "                  #print(id(d_t_p_c))\n",
        "                  temp_p_f=d_t_p_c\n",
        "                  swarms['sub-swarm '+str(n_i)]['fit-par-t-1'][n_j]=swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]\n",
        "                  swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]=temp_p_f\n",
        "                  '''print(\"Fitness of particle in t,t-1 iterations \",swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j],swarms['sub-swarm '+str(n_i)]['fit-par-t-1'][n_j])\n",
        "                  '''\n",
        "                except:\n",
        "                  #print(swarms['sub-swarm '+str(n_i)]['fit-par-t-1'])\n",
        "                  #print(n_i,n_j,SN[n_i])\n",
        "                  print(\"e2\")\n",
        "                  pass\n",
        "            else:\n",
        "              #print(\"Eureka!!\")\n",
        "              try:\n",
        "                swarms['sub-swarm '+str(n_i)]['fit-par-t-1'][n_j]=swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]\n",
        "                swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]= swarms['sub-swarm '+str(n_i)]['fit-par-t'][n_j]\n",
        "              except:\n",
        "                print(\"err\")\n",
        "                pass\n",
        "              curr_eval=curr_eval+1\n",
        "              pass\n",
        "            swarms['sub-swarm common rel_con'][n_i]=relative_convergence(swarms,n_i,i,curr_eval)\n",
        "            swarms['sub-swarm common rel_div'][n_i]=relative_divergence(swarms,n_i)\n",
        "          print(\"Relative Convergence:\")\n",
        "          print(swarms['sub-swarm common rel_con'])\n",
        "          print(\"Relative Divergence:\")\n",
        "          print(swarms['sub-swarm common rel_div'])\n",
        "            ##\n",
        "            #print(i['gbest'])\n",
        "          adaptive_sub_swarm_size(SN,swarms,N,M,max_eval,curr_eval)\n",
        "          #if c%5==0:\n",
        "          if True:\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name)\n",
        "            start=time.time()\n",
        "          c=c+1\n",
        "        plot_pdf('naive bayes')\n",
        "        plot_pdf('tree')\n",
        "        plot_pdf('svm_linear')\n",
        "        plot_pdf('knn')\n",
        "\n",
        "        print(\"For Naive Bayes\")\n",
        "        mean_clf('naive bayes')\n",
        "        print(\"\\nFor Tree\")\n",
        "        mean_clf('tree')\n",
        "        print(\"\\nFor SVM Linear\")\n",
        "        mean_clf('svm_linear')\n",
        "        print(\"\\nFor KNN\")\n",
        "        mean_clf('knn')\n",
        "        # done=time.time()\n",
        "        # elapsed=done-start-(extra_time)\n",
        "        #print((\"Total time taken: %f\")%(elapsed/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_cILROINDsm",
        "outputId": "628a72dd-606d-48e9-f925-9a6ee11f10bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG44IVSAzbE2"
      },
      "source": [
        "# import numpy as np\n",
        "# arr = np.array([[-0.30565392, -0.96605562],\n",
        "#                 [ 0.85331367, -2.62963495],\n",
        "#                 [ 0.87839643, -0.28283675],\n",
        "#                 [ 0.72676698,  0.93213482],\n",
        "#                 [-0.52007354,  0.27752806],\n",
        "#                 [-0.08701666,  0.22764316],\n",
        "#                 [-1.78897817,  0.50737573],\n",
        "#                 [ 0.62260038, -1.96012161],\n",
        "#                 [-1.98231706,  0.36523876],\n",
        "#                 [-1.07587382, -2.3022289 ]])\n",
        "# print(arr[:, 1].argsort())\n",
        "# https://stackoverflow.com/questions/22698687/how-to-sort-2d-array-numpy-ndarray-based-to-the-second-column-in-python/22699957"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK1EK9HwX9pP"
      },
      "source": [
        "# Utilitiy functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4CosC-aKfgk"
      },
      "source": [
        "def plot_pdf(clf):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from matplotlib.backends.backend_pdf import PdfPages\n",
        "  import pandas as pd\n",
        "  from matplotlib.pyplot import figure\n",
        "  figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "  df=pd.read_csv(('%s/%s.csv')%(results_csv,clf))\n",
        "  colunms=df.columns\n",
        "  i=0\n",
        "  with PdfPages(('%s/%s.pdf')%(results_graphs,clf)) as pdf:\n",
        "    while True:\n",
        "      if i>=df.shape[1]:\n",
        "        break\n",
        "      df[df.columns[i:i+1]].plot(subplots=True)\n",
        "      pdf.savefig()\n",
        "      i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeYuboybWMBf"
      },
      "source": [
        "def mean_clf(clf):\n",
        "  df=pd.read_csv(('%s/%s.csv')%(results_csv,clf))\n",
        "  temp=((df.mean(axis = 0,skipna = True))) \n",
        "  print(temp)\n",
        "  #(pd.DataFrame(df.mean(axis = 0,skipna = True)).T).to_csv((\"%stest.csv\")%(clf))\n",
        "  temp.to_csv((\"%s/%s_average.csv\")%(results_csv,clf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjfLj6bCQOcg"
      },
      "source": [
        "# def normalizer(X_particle):\n",
        "#   return (X_particle-X_particle.min())/(X_particle.max()-X_particle.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5e4mzWLYD2g"
      },
      "source": [
        "def normalizer(X_particle):\n",
        "  X_particle[X_particle>1.0]=1\n",
        "  X_particle[X_particle<0.0]=0\n",
        "  return X_particle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mbhhu40j6J0"
      },
      "source": [
        "def add_result_to_csv(score,fn,nsf):\n",
        "  import csv\n",
        "  fieldnames = ['Iteration','M','# of particles','Total # of features','# of features selected','Running Time of FS','Fit-time','Score-time','Train_Accuracy','Test_Accuracy','Train_precision','Test_precision','Train_recall','Test_recall','Train_F-measure','Test_F-measure','Train_AUC','Test_AUC']\n",
        "  file_exists = os.path.isfile(('%s.csv')%(fn))\n",
        "  with open(('%s.csv')%(fn), 'a', newline='') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      if not file_exists:\n",
        "        writer.writeheader() \n",
        "      for i in score.keys():\n",
        "        score[i]=np.mean(score[i])\n",
        "      try:\n",
        "        writer.writerow({'Iteration':c, 'M':M,'# of particles':np.sum(SN),'Total # of features':n_s_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro'],'Train_AUC':score['train_roc_auc_ovr'],'Test_AUC':score['test_roc_auc_ovr']})\n",
        "      except:\n",
        "        writer.writerow({'Iteration':c, 'M':M,'# of particles':np.sum(SN),'Total # of features':n_s_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro']})\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-f4gLhQXZz0"
      },
      "source": [
        "def nb_tree_svm(df):\n",
        "  X=df.values\n",
        "  y=X[:,-1]\n",
        "  X=X[:,:-1]\n",
        "  from sklearn.model_selection import KFold, cross_val_score\n",
        "  from sklearn import svm\n",
        "  from sklearn import tree\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  from sklearn.model_selection import StratifiedKFold\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.model_selection import KFold\n",
        "  #from sklearn.calibration import CalibratedClassifiertCV\n",
        "  #from sklearn.multiclass import OneVsRestClassifier\n",
        "  \n",
        "  try:\n",
        "    #print(e)\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    clf = GaussianNB()\n",
        "    scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "    #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "    nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  except :\n",
        "    try:\n",
        "      k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      clf = GaussianNB()\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      clf = GaussianNB()\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  fn=('%s/naive bayes')%(results_csv)\n",
        "  add_result_to_csv(nb_score,fn,df.shape[1]-1)\n",
        "  #########################################################################################################\n",
        "  try:\n",
        "    #print(e)\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "    #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "    clf = tree.DecisionTreeClassifier()\n",
        "    tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  except:\n",
        "    try:\n",
        "      k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      clf = tree.DecisionTreeClassifier()\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      #print(e)\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      clf = tree.DecisionTreeClassifier()\n",
        "      tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  fn=('%s/tree')%(results_csv)\n",
        "  add_result_to_csv(tree_score,fn,df.shape[1]-1)\n",
        "  #######################################################################################################\n",
        "  if False:\n",
        "    try:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      clf = svm.SVC(kernel='linear',probability=True)\n",
        "      svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      try:\n",
        "        k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "      except:\n",
        "        k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    fn=('%s/svm_linear')%(results_csv)\n",
        "    add_result_to_csv(svm_score,fn,df.shape[1]-1)\n",
        "  else:\n",
        "    try:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      #clf = svm.SVC(kernel='linear',probability=True)\n",
        "      clf=svm.LinearSVC()\n",
        "      svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      try:\n",
        "        k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "      except:\n",
        "        k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    fn=('%s/svm_linear')%(results_csv)\n",
        "    add_result_to_csv(svm_score,fn,df.shape[1]-1)\n",
        "  ########################################################################################################\n",
        "  try:\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "    #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "    clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "    knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  except:\n",
        "    try:\n",
        "      k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "      knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "      knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  fn=('%s/knn')%(results_csv)\n",
        "  add_result_to_csv(knn_score,fn,df.shape[1]-1)\n",
        "  return nb_score,tree_score,svm_score,knn_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR80eHkeYIrb"
      },
      "source": [
        "# def nb_tree_svm(df):\n",
        "#   X=df.values\n",
        "#   y=X[:,-1]\n",
        "#   X=X[:,:-1]\n",
        "#   from sklearn.model_selection import KFold, cross_val_score\n",
        "#   from sklearn import svm\n",
        "#   from sklearn import tree\n",
        "#   from sklearn.naive_bayes import GaussianNB\n",
        "#   from sklearn.neighbors import KNeighborsClassifier\n",
        "#   from sklearn.model_selection import cross_val_score\n",
        "#   from sklearn.model_selection import cross_validate\n",
        "#   from sklearn.model_selection import StratifiedKFold\n",
        "#   from sklearn.neighbors import KNeighborsClassifier\n",
        "#   from sklearn.model_selection import KFold\n",
        "#   #from sklearn.calibration import CalibratedClassifiertCV\n",
        "#   #from sklearn.multiclass import OneVsRestClassifier\n",
        "#   if True:\n",
        "#     try:\n",
        "#       #print(e)\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       clf = GaussianNB()\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    \n",
        "#     except :\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         clf = GaussianNB()\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         clf = GaussianNB()\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     #########################################################################################################\n",
        "#     try:\n",
        "#       #print(e)\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       clf = tree.DecisionTreeClassifier()\n",
        "#       tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     except:\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         clf = tree.DecisionTreeClassifier()\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         #print(e)\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         clf = tree.DecisionTreeClassifier()\n",
        "#         tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     #######################################################################################################\n",
        "#     try:\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       clf = svm.SVC(kernel='linear',probability=True)\n",
        "#       svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     except:\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         clf = svm.SVC(kernel='linear',probability=True)\n",
        "#         svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         clf = svm.SVC(kernel='linear',probability=True)\n",
        "#         svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     ########################################################################################################\n",
        "#     try:\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "#       knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     except:\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "#         knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "#         knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#   else:\n",
        "#     try:\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       clf = GaussianNB()\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)  \n",
        "#     except :\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         clf = GaussianNB()\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#         nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         #print(e)\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         clf = GaussianNB()\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     #########################################################################################################\n",
        "#     try:\n",
        "#       #print(e)\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       clf = tree.DecisionTreeClassifier()\n",
        "#       tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     except:\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         clf = tree.DecisionTreeClassifier()\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#         tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         #print(e)\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         clf = tree.DecisionTreeClassifier()\n",
        "#         tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     #######################################################################################################\n",
        "#     try:\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       clf = svm.SVC(kernel='linear',probability=True)\n",
        "#       svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     except:\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#         clf = svm.SVC(kernel='linear',probability=True)\n",
        "#         svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         clf = svm.SVC(kernel='linear',probability=True)\n",
        "#         svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     ########################################################################################################\n",
        "#     try:\n",
        "#       k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#       #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#       scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#       clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "#       knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#     except:\n",
        "#       try:\n",
        "#         k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc']\n",
        "#         clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "#         knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#       except:\n",
        "#         k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#         scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "#         #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "#         clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "#         knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "#   fn=('%s/naive bayes')%(results_csv)\n",
        "#   add_result_to_csv(nb_score,fn,df.shape[1]-1)\n",
        "#   fn=('%s/tree')%(results_csv)\n",
        "#   add_result_to_csv(tree_score,fn,df.shape[1]-1)\n",
        "#   fn=('%s/svm_linear')%(results_csv)\n",
        "#   add_result_to_csv(svm_score,fn,df.shape[1]-1)\n",
        "#   fn=('%s/knn')%(results_csv)\n",
        "#   add_result_to_csv(knn_score,fn,df.shape[1]-1)\n",
        "#   return nb_score,tree_score,svm_score,knn_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6gGlzA8d-Qb"
      },
      "source": [
        "def knn(X,y):\n",
        "  \"\"\"knn with Leave one out  cv\"\"\"\n",
        "  if X.shape[1]<1:\n",
        "    return 0\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  from sklearn.model_selection import LeaveOneOut\n",
        "  #import numpy as np\n",
        "  k_fold = LeaveOneOut()\n",
        "  clf = KNeighborsClassifier(n_neighbors=1,p=2)\n",
        "  #return np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1))\n",
        "  #scoring = ['balanced_accuracy']\n",
        "  scoring = ['accuracy']\n",
        "  knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1)\n",
        "  #return np.mean(knn_score['test_balanced_accuracy'])\n",
        "  return np.mean(knn_score['test_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T72kwQ1dIMd"
      },
      "source": [
        "# def knn(X,y):\n",
        "#   \"\"\"knn with 10 fold cv\"\"\"\n",
        "#   if X.shape[1]<1:\n",
        "#     return 0\n",
        "#   from sklearn.neighbors import KNeighborsClassifier\n",
        "#   from sklearn.model_selection import cross_val_score\n",
        "#   import numpy as np\n",
        "#   #create a new KNN model\n",
        "#   #knn_cv = KNeighborsClassifier(n_neighbors=5)\n",
        "#   knn_cv = KNeighborsClassifier(n_neighbors=3,p=1,n_jobs=-1)\n",
        "#   #train model with cv of 5 \n",
        "#   cv_scores = cross_val_score(knn_cv, X, y, cv=10,n_jobs=-1)\n",
        "#   #print each cv score (accuracy) and average them\n",
        "#   #print(cv_scores)\n",
        "#   #print(‘cv_scores mean:{}’.format(np.mean(cv_scores)))\n",
        "#   return np.mean(cv_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJoMHPttYQh2"
      },
      "source": [
        "## fitness functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCp8qta7EMau"
      },
      "source": [
        "def fitness_particle_re(swarms,i,j,curr_eval):\n",
        "  curr_eval=curr_eval+1\n",
        "  #th=0.6\n",
        "  sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "  p_elite_comb=[]\n",
        "  for k in range(0,len(SN)):\n",
        "    if k ==i:\n",
        "      th1=th2=1\n",
        "      # for fs_index in range(0,len(U[k])):\n",
        "      #   # if sub_swarm_particles['particle '+str(j)]['cur_pos'][fs_index] == th1:\n",
        "      #   #   #print(\"yes\")\n",
        "      #   #   p_elite_comb.append(U[k][fs_index])\n",
        "      tr_ind=sub_swarm_particles['particle '+str(j)]['best_pos'] == th1\n",
        "      # print(tr_ind)\n",
        "      p_elite_comb.extend(U[k][tr_ind])\n",
        "    else:\n",
        "      try:\n",
        "        ## for NO modify\n",
        "        th=1\n",
        "        # for fs_index in range(0,len(U[k])):\n",
        "        #   if  swarms['sub-swarm '+str(i)]['gbest'][fs_index] == th:\n",
        "        #     #print(\"yes\")\n",
        "        #     p_elite_comb.append(U[k][fs_index])\n",
        "        # print(tr_ind)\n",
        "        tr_ind=swarms['sub-swarm '+str(k)]['gbest'] == th\n",
        "        p_elite_comb.extend(U[k][tr_ind])\n",
        "      except:\n",
        "        #print(\"ok\")\n",
        "        print(\"e3\")\n",
        "        pass\n",
        " #print(p_elite_comb)\n",
        "  p_elite_comb=np.array(p_elite_comb)\n",
        "  p_elite_comb=p_elite_comb.astype(int)\n",
        "  y_c_dim=y.reshape(-1, 1) \n",
        "  X_fs=X[:,p_elite_comb]\n",
        "  p_acc=knn(X_fs,y_c_dim)\n",
        "  return p_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clsYguiBYP0G"
      },
      "source": [
        "def fitness_particle(swarms,i,j,curr_eval):\n",
        "  curr_eval=curr_eval+1\n",
        "  #th=0.6\n",
        "  sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "  p_elite_comb=[]\n",
        "  for k in range(0,len(SN)):\n",
        "    if k ==i:\n",
        "      th1=th2=1\n",
        "      # for fs_index in range(0,len(U[k])):\n",
        "      #   # if sub_swarm_particles['particle '+str(j)]['cur_pos'][fs_index] == th1:\n",
        "      #   #   #print(\"yes\")\n",
        "      #   #   p_elite_comb.append(U[k][fs_index])\n",
        "      tr_ind=sub_swarm_particles['particle '+str(j)]['cur_pos'] == th1\n",
        "      # print(tr_ind)\n",
        "      p_elite_comb.extend(U[k][tr_ind])\n",
        "    else:\n",
        "      try:\n",
        "        ## for NO modify\n",
        "        th=1\n",
        "        # for fs_index in range(0,len(U[k])):\n",
        "        #   if  swarms['sub-swarm '+str(i)]['gbest'][fs_index] == th:\n",
        "        #     #print(\"yes\")\n",
        "        #     p_elite_comb.append(U[k][fs_index])\n",
        "        # print(tr_ind)\n",
        "        tr_ind=swarms['sub-swarm '+str(k)]['gbest'] == th\n",
        "        p_elite_comb.extend(U[k][tr_ind])\n",
        "      except:\n",
        "        #print(\"ok\")\n",
        "        print(\"e3\")\n",
        "        pass\n",
        " #print(p_elite_comb)\n",
        "  p_elite_comb=np.array(p_elite_comb)\n",
        "  p_elite_comb=p_elite_comb.astype(int)\n",
        "  y_c_dim=y.reshape(-1, 1) \n",
        "  X_fs=X[:,p_elite_comb]\n",
        "  p_acc=knn(X_fs,y_c_dim)\n",
        "  return p_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCUWsOAYWnY"
      },
      "source": [
        "###addede fitness finder for gbest only\n",
        "def fitness_gbest(swarms,i,curr_eval):\n",
        "  #th=0.6\n",
        "  curr_eval=curr_eval+1\n",
        "  sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "  g_elite_comb=[]\n",
        "  for k in range(0,len(SN)):\n",
        "    if k == i:\n",
        "      th1=th2=1\n",
        "      # for fs_index in range(0,len(U[k])):\n",
        "      #   if swarms['sub-swarm '+str(i)]['gbest'][fs_index] == th2:\n",
        "      #     g_elite_comb.append(U[k][fs_index])\n",
        "      tr_ind=swarms['sub-swarm '+str(i)]['gbest'] == th2\n",
        "      g_elite_comb.extend(U[k][tr_ind])\n",
        "    else:\n",
        "      try:\n",
        "        th=1\n",
        "        # for fs_index in range(0,len(U[k])):\n",
        "        #   if swarms['sub-swarm '+str(i)]['gbest'][fs_index] == th:\n",
        "        #     g_elite_comb.append(U[k][fs_index])\n",
        "        tr_ind=swarms['sub-swarm '+str(k)]['gbest'] == th\n",
        "        g_elite_comb.extend(U[k][tr_ind])\n",
        "      except:\n",
        "        #print(\"ok\")\n",
        "        print(\"e4\")\n",
        "        pass\n",
        "  #print(g_elite_comb)\n",
        "  g_elite_comb=np.array(g_elite_comb)\n",
        "  g_elite_comb=g_elite_comb.astype(int)\n",
        "  y_c_dim=y.reshape(-1, 1) \n",
        "  X_fs=X[:,g_elite_comb]\n",
        "  g_acc=knn(X_fs,y_c_dim)\n",
        "  #print(g_acc)\n",
        "  #print(pb_acc)\n",
        "  return g_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLEWRcTvYhH2"
      },
      "source": [
        "## relative convergence and divergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm9okp4QYgZQ"
      },
      "source": [
        "def relative_convergence(swarms,i,sub_swarm,curr_eval):\n",
        "  t1=sub_swarm['fbest t']\n",
        "  t2=sub_swarm['fbest t-1']\n",
        "  t3=sub_swarm['fbest t-2']\n",
        "  try:\n",
        "    rel_con=((t1-t2)-(t2-t3))/2\n",
        "  except:\n",
        "    rel_con=0\n",
        "  sub_swarm['fbest t-2']=t2\n",
        "  sub_swarm['fbest t-1']=t1\n",
        "  #sub_swarm['fbest t']=fitness_gbest(swarms,i,curr_eval)\n",
        "  sub_swarm['fbest t']=swarms['sub-swarm '+str(i)]['gbest-val']\n",
        "  return rel_con"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N_xEjPRYlJi"
      },
      "source": [
        "def relative_divergence(swarms,i):\n",
        "  term1=swarms['sub-swarm '+str(i)]['fit-par-t']\n",
        "  term2=swarms['sub-swarm '+str(i)]['fit-par-t-1']\n",
        "  etta=0.5\n",
        "  return np.mean((term1-np.mean(term1)))-etta*(np.mean(term2-np.mean(term2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0biA2bDKYnI1"
      },
      "source": [
        "## adaptive size mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irXo4dylYsrS"
      },
      "source": [
        "def adaptive_sub_swarm_size(SN,swarms,N,M,max_eval,curr_eval):\n",
        "  rel_con_arr=swarms['sub-swarm common rel_con']\n",
        "  rel_div_arr=swarms['sub-swarm common rel_div']\n",
        "  temp_SN=np.zeros(len(SN)) #it holds the temporary calculated sizes of all sub-swarms\n",
        "  for i in range(0,len(SN)):\n",
        "    rel_con=rel_con_arr[i]\n",
        "    rel_div=rel_div_arr[i]\n",
        "    if rel_con<0 and rel_div>0 and SN[i]>N/M:\n",
        "      temp_SN[i]=SN[i]-0.5*((rel_con_arr.max()-rel_con)/(rel_con_arr.max()-rel_con_arr.min())+(rel_div_arr.max()-rel_div)/(rel_div_arr.max()-rel_div_arr.min())*(SN[i]-max(SN)))\n",
        "      #SN[i]=SN[i]-0.5*((rel_con_arr.max()-rel_con)/(rel_con_arr.max()-rel_con_arr.min())+(rel_div_arr.max()-rel_div)/(rel_div_arr.max()-rel_div_arr.min())*(SN[i]-SN.max())\n",
        "    elif rel_con>0 and rel_div<0 and SN[i]<N/M:\n",
        "      temp_SN[i]=SN[i]+((rel_con_arr.max()-rel_con)/(rel_con_arr.max()-rel_con_arr.min())*(rel_div_arr.max()-rel_div)/(rel_div_arr.max()-rel_div_arr.min())*(max(SN)-SN[i]))\n",
        "      #SN[i]=SN[i]+((rel_con_arr.max()-rel_con)/(rel_con_arr.max()-rel_con_arr.min())*(rel_div_arr.max()-rel_div)/(rel_div_arr.max()-rel_div_arr.min())*(SN.max()-SN[i])\n",
        "    elif rel_con<0 and rel_div<0 and SN[i]<N/M :\n",
        "      if curr_eval<0.5*max_eval:\n",
        "        temp_SN[i]=SN[i]+0.5*((rel_con_arr.max()-rel_con)/(rel_con_arr.max()-rel_con_arr.min())+(rel_div_arr.max()-rel_div)/(rel_div_arr.max()-rel_div_arr.min())*(max(SN)-SN[i]))\n",
        "        #SN[i]=SN[i]+0.5*((rel_con_arr.max()-rel_con)/(rel_con_arr.max()-rel_con_arr.min())+(rel_div_arr.max()-rel_div)/(rel_div_arr.max()-rel_div_arr.min())*(SN.max()-SN[i])\n",
        "    else:\n",
        "      temp_SN[i]=SN[i]\n",
        "  temp_delta=temp_SN-SN\n",
        "  #print(temp_delta)\n",
        "  for i in range(len(temp_delta)):\n",
        "    try:\n",
        "      #print(temp_delta[i])\n",
        "      if int(temp_delta[i])<0:\n",
        "        print(\"Particle Deletion Strategy called for sub-swarm \"+str(i)+\" with delta: \"+str(int(temp_delta[i])))\n",
        "        particle_deletion(swarms,i,SN,abs(temp_delta[i]))\n",
        "        #pass\n",
        "      elif int(temp_delta[i])>0:\n",
        "        print(\"Particle Generation Strategy called for sub-swarm \"+str(i))\n",
        "        particle_generation(swarms,i,SN,abs(temp_delta[i]),curr_eval)\n",
        "        #pass\n",
        "        #print(\"Hello\")\n",
        "      #print(i)\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLx7rxycYvR3"
      },
      "source": [
        "## particle deletion startegies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wyWveiXY1C6"
      },
      "source": [
        "def fitness_guided_b_clu_str(swarms,i,SN):\n",
        "  #fitness_particles_swarm_i=np.zeros(SN[i])\n",
        "  fitness_particles_swarm_i=np.zeros(len(swarms['sub-swarm '+str(i)]['particles']))\n",
        "  #print(swarms['sub-swarm '+str(i)]['particles'])\n",
        "  #print(len(swarms['sub-swarm '+str(i)]['particles']))\n",
        "  for j in range(0,len(swarms['sub-swarm '+str(i)]['particles'])):\n",
        "    try:\n",
        "      fitness_particles_swarm_i[j]=fitness_particle(swarms,i,j)\n",
        "    except:\n",
        "      #print(i,j)\n",
        "      print(\"e5\")\n",
        "      pass\n",
        "  F1=fitness_particles_swarm_i.argsort(kind='mergesort')\n",
        "  F1=F1[::-1]\n",
        "  #print(F1)\n",
        "  k=1 #i=1 in paper \n",
        "  sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "  ind_min=0\n",
        "  val_min=float('inf')\n",
        "  c_rem=0\n",
        "  cluster_res=[]\n",
        "  while len(swarms['sub-swarm '+str(i)]['particles'])-c_rem>2:\n",
        "    #print(len(swarms['sub-swarm '+str(i)]['particles']))\n",
        "    if F1[k]!=-1:\n",
        "      cur_particle=sub_swarm_particles['particle '+str(F1[k])]['cur_pos']\n",
        "      val_min=float('inf')\n",
        "      for i_F1 in range(k+1,len(F1)):\n",
        "        if F1[i_F1]!=-1:\n",
        "          try:\n",
        "            dis=np.abs(cur_particle-sub_swarm_particles['particle '+str(F1[i_F1])]['cur_pos']).sum()\n",
        "          except:\n",
        "            print(i_F1)\n",
        "            print(\"e6\")\n",
        "            pass\n",
        "          if dis<val_min:\n",
        "            val_min=dis\n",
        "            ind_min=i_F1\n",
        "      cluster_res.append([F1[k],F1[ind_min]])\n",
        "      F1[ind_min]=-1\n",
        "      c_rem=c_rem+2\n",
        "    #line side right\n",
        "    k=k+1\n",
        "  return cluster_res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZJGzr9gY1rK"
      },
      "source": [
        "def particle_deletion(swarms,i,SN,delta_N):\n",
        "  chk=0\n",
        "  while True:\n",
        "    #print(chk)\n",
        "    chk=chk+1\n",
        "    if chk>1000:\n",
        "      print(\"feeling drain\")\n",
        "      SN[i]=len(swarms['sub-swarm '+str(i)]['particles'])\n",
        "      return\n",
        "    ele_del=[]\n",
        "    cluster_res=fitness_guided_b_clu_str(swarms,i,SN)\n",
        "    #print(cluster_res)\n",
        "    print(\"current  NO. of Deleted particles : \"+str(int(SN[i]-len(swarms['sub-swarm '+str(i)]['particles']))))\n",
        "    crowd_deg=np.zeros(len(cluster_res))\n",
        "    dc=0\n",
        "    for k in cluster_res:\n",
        "      t_i=swarms['sub-swarm '+str(i)]['particles']\n",
        "      #print(k[0])\n",
        "      dis=np.abs(t_i['particle '+str(k[0])]['cur_pos']-t_i['particle '+str(k[1])]['cur_pos']).sum()\n",
        "      crowd_deg[dc]=dis\n",
        "      dc=dc+1\n",
        "    min_par_val=min(len(swarms['sub-swarm '+str(i)]['particles'])-(SN[i]-delta_N),len(cluster_res))\n",
        "    F1=crowd_deg.argsort(kind='mergesort')\n",
        "    #print(min_par_val)\n",
        "    tot_len=len(swarms['sub-swarm '+str(i)]['particles'])\n",
        "    for k in range(int(min_par_val)):\n",
        "      #print(k)\n",
        "      p1_fit=fitness_particle(swarms,i,cluster_res[k][0])\n",
        "      p2_fit=fitness_particle(swarms,i,cluster_res[k][1])\n",
        "      if p1_fit<p2_fit:\n",
        "        del(swarms['sub-swarm '+str(i)]['particles']['particle '+str(cluster_res[k][0])])\n",
        "        ele_del.append(cluster_res[k][0])\n",
        "      else:\n",
        "        del(swarms['sub-swarm '+str(i)]['particles']['particle '+str(cluster_res[k][1])])\n",
        "        ele_del.append(cluster_res[k][1])\n",
        "    np.delete(swarms['sub-swarm '+str(i)]['fit-par-t'],ele_del)\n",
        "    np.delete(swarms['sub-swarm '+str(i)]['fit-par-t-1'],ele_del)\n",
        "    p=0\n",
        "    s_s_p={}\n",
        "    #print(len(swarms['sub-swarm '+str(i)]['particles']))\n",
        "    for k in range(tot_len):\n",
        "      #print(k)\n",
        "      t_i=swarms['sub-swarm '+str(i)]['particles']\n",
        "      try:\n",
        "        s_s_p['particle '+str(p)]={}\n",
        "        s_s_p['particle '+str(p)]['cur_pos']=t_i['particle '+str(k)]['cur_pos']\n",
        "        s_s_p['particle '+str(p)]['cur_vel']=t_i['particle '+str(k)]['cur_vel']\n",
        "        s_s_p['particle '+str(p)]['best_pos']=t_i['particle '+str(k)]['best_pos']\n",
        "        p=p+1\n",
        "      except:\n",
        "        print(\"e7\")\n",
        "        del(s_s_p['particle '+str(p)])\n",
        "        pass\n",
        "    swarms['sub-swarm '+str(i)]['particles']=s_s_p\n",
        "    #print(swarms['sub-swarm '+str(i)]['particles']['particle '+str(0)]['cur_pos'])\n",
        "    #print(len(swarms['sub-swarm '+str(i)]['particles']),SN[i]-delta_N)\n",
        "    if len(swarms['sub-swarm '+str(i)]['particles'])==SN[i]-delta_N:\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t'],ele_del)\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t-1'],ele_del)\n",
        "      SN[i]=SN[i]-delta_N\n",
        "      return \n",
        "    else:\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t'],ele_del)\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t-1'],ele_del)\n",
        "      if len(swarms['sub-swarm '+str(i)]['particles'])<SN[i]-delta_N:\n",
        "        print(\"bug\")\n",
        "        return\n",
        "      if len(swarms['sub-swarm '+str(i)]['particles'])==2:\n",
        "        SN[i]=2\n",
        "        return\n",
        "    # else:\n",
        "    #   print(swarms['sub-swarm '+str(i)]['particles'])\n",
        "    #   return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FVp1cfHY9b2"
      },
      "source": [
        "## particle generation startegy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0cyW64iY438"
      },
      "source": [
        "def particle_generation(swarms,i,SN,delta_N,curr_eval):\n",
        "  import math\n",
        "  import random\n",
        "  F_F=swarms['sub-swarm '+str(i)]['fit-par-t'].argsort(kind='mergesort')\n",
        "  #print(delta_N+1)\n",
        "  ind_sli=int(delta_N+1)\n",
        "  F_F=F_F[:ind_sli]\n",
        "  #F1=F1[::-1]\n",
        "  sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "  #print(sub_swarm_particles)\n",
        "  for p in range(int(delta_N)):\n",
        "    h=max(1,math.floor(0.1*random.uniform(0,1)*len(U[i]))) \n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]={}\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']=np.random.uniform(0,1,len(U[i]))\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos'][:h]=1\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']=0.5*(sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']+swarms['sub-swarm '+str(i)]['gbest'])\n",
        "    #sub_swarm_particles['particle '+str(SN[i]+p)]['cur_vel']=np.random.uniform(0,1,len(U[i]))\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['best_pos']=sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']\n",
        "    #print(sub_swarm_particles)\n",
        "    d_t=sub_swarm_particles['particle '+str(SN[i]+p)]['best_pos-val']=fitness_particle(swarms,i,SN[i]+p,curr_eval)\n",
        "    if (swarms['sub-swarm '+str(i)]['gbest'] is None )or (d_t>swarms['sub-swarm '+str(i)]['gbest-val']) :\n",
        "      swarms['sub-swarm '+str(i)]['gbest']=sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']\n",
        "      swarms['sub-swarm '+str(i)]['gbest-val']=d_t\n",
        "    temp_p_f=d_t\n",
        "    # swarms['sub-swarm '+str(i)]['fit-par-t'][SN[i]+p]=temp_p_f\n",
        "    # swarms['sub-swarm '+str(i)]['fit-par-t-1'][SN[i]+p]=temp_p_f\n",
        "    #print(swarms['sub-swarm '+str(i)]['fit-par-t'])\n",
        "    swarms['sub-swarm '+str(i)]['fit-par-t']=np.append(swarms['sub-swarm '+str(i)]['fit-par-t'],temp_p_f)\n",
        "    swarms['sub-swarm '+str(i)]['fit-par-t-1']=np.append(swarms['sub-swarm '+str(i)]['fit-par-t-1'],temp_p_f)\n",
        "    #print(swarms['sub-swarm '+str(i)]['fit-par-t'])\n",
        "    #print(temp_p_f)\n",
        "    #print(\"hello\")\n",
        "  SN[i]=SN[i]+int(delta_N)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCsnh5U4ZC2A"
      },
      "source": [
        "## saving functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Xa85ajZFcQ"
      },
      "source": [
        "def save_obj(obj, name ):\n",
        "    with open( name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "def load_obj(name ):\n",
        "    with open( name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "def save_file(ds_name):\n",
        "  print(\"Save reduced dataset at iteration: \"+str(c))\n",
        "  g_elite_comb=[]\n",
        "  th=1.0\n",
        "  for k in range(0,len(SN)):\n",
        "    # for fs_index in range(0,len(U[k])):\n",
        "    #   try:\n",
        "    #     #th=0.6*Fim[k]\n",
        "    #     #th=0.6*(1-(Fim[k]/sum_fim))\n",
        "    #     if swarms['sub-swarm '+str(k)]['gbest'][fs_index] == th:\n",
        "    #       g_elite_comb.append(U[k][fs_index])\n",
        "    #   except:\n",
        "    #     pass\n",
        "    tr_ind=swarms['sub-swarm '+str(k)]['gbest'] == th\n",
        "    #print(swarms['sub-swarm '+str(k)]['gbest'])\n",
        "    #print(tr_ind)\n",
        "    #print(swarms['sub-swarm '+str(k)]['gbest'])\n",
        "    #print(U[k])\n",
        "    #print(U[k][tr_ind])\n",
        "    g_elite_comb.extend(U[k][tr_ind])\n",
        "    #print(len(U[k]),len(U[k][tr_ind]))\n",
        "  print(\"Total No. of Features:\",n_features)\n",
        "  try:\n",
        "    print(\"NO. of Features Selected:\",len(g_elite_comb))\n",
        "    print(\"Selected Features: \",g_elite_comb)\n",
        "  except:\n",
        "    print(\"e8\")\n",
        "    pass\n",
        "  target_no=n_features\n",
        "  g_elite_comb.append(target_no)\n",
        "  g_elite_comb=np.array(g_elite_comb)\n",
        "  g_elite_comb=g_elite_comb.astype(int)\n",
        "  X_gfs=X[:,g_elite_comb]\n",
        "  \n",
        "  #print(X_gfs)\n",
        "  DF = pd.DataFrame(X_gfs)\n",
        "  DF.columns=df.columns[g_elite_comb]\n",
        "  #DF['class']=y\n",
        "  \n",
        "  #print time\n",
        "  # now = datetime.now()\n",
        "  # current_time = now.strftime(\"%H:%M:%S\")\n",
        "  # print(\"Current Time =\", current_time)\n",
        "  #done = time.time()\n",
        "  #elapsed = done - start\n",
        "  DF.to_csv(('%s/iteration%d.csv')%(reduced_datasets,c),index=False) \n",
        "  name=('%s/iteration%d')%(temp_storage,c)\n",
        "  #save_obj(swarms, name )\n",
        "  nbs,tres,svms,knns=nb_tree_svm(DF)\n",
        "  print(\"Naive Bayes Train-accuracy: \",nbs['train_accuracy'],\" Test-accuracy: \",nbs['test_accuracy'])\n",
        "  print(\"Trees Train-accuracy: \",tres['train_accuracy'],\" Test-accuracy: \",tres['test_accuracy'])\n",
        "  print(\"SVM Linear Train-accuracy: \",svms['train_accuracy'],\" Test-accuracy: \",svms['test_accuracy'])\n",
        "  print(\"Knn Train-accuracy: \",knns['train_accuracy'],\" Test-accuracy: \",knns['test_accuracy'])\n",
        "  # import csv   \n",
        "  # fields=[ds_name,c,n_features,len(g_elite_comb),elapsed/60,len(SN),np.sum(SN),nbs,tres,svms,knns]\n",
        "  # with open(r'/content/drive/My Drive/FYP/Result/VS_CCPSO_Result.csv', 'a') as f:\n",
        "  #   writer = csv.writer(f)\n",
        "  #   writer.writerow(fields)\n",
        "  #print(\"Time Elapsed in (min):\",elapsed/60)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_krtHc9sM2bF"
      },
      "source": [
        "## modied codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poylfyngM2Dj"
      },
      "source": [
        "def particle_generation(swarms,i,SN,delta_N,curr_eval):\n",
        "  import math\n",
        "  import random\n",
        "  F_F=swarms['sub-swarm '+str(i)]['fit-par-t'].argsort(kind='mergesort')\n",
        "  #print(delta_N+1)\n",
        "  ind_sli=int(delta_N+1)\n",
        "  F_F=F_F[:ind_sli]\n",
        "  #F1=F1[::-1]\n",
        "  sub_swarm_particles=swarms['sub-swarm '+str(i)]['particles']\n",
        "  #print(sub_swarm_particles)\n",
        "  for p in range(int(delta_N)):\n",
        "    h=max(1,math.floor(0.1*random.uniform(0,1)*len(U[i]))) \n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]={}\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']=np.random.uniform(0,1,len(U[i]))\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos'][:h]=1\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']=0.5*(sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']+swarms['sub-swarm '+str(i)]['gbest'])\n",
        "    #sub_swarm_particles['particle '+str(SN[i]+p)]['cur_vel']=np.random.uniform(0,1,len(U[i]))\n",
        "    sub_swarm_particles['particle '+str(SN[i]+p)]['best_pos']=sub_swarm_particles['particle '+str(SN[i]+p)]['cur_pos']\n",
        "    #print(sub_swarm_particles)\n",
        "    d_t=sub_swarm_particles['particle '+str(SN[i]+p)]['best_pos-val']=fitness_particle(swarms,i,SN[i]+p,curr_eval)\n",
        "    if (d_t>gbest_tracer):\n",
        "      '''print(\"Global best position,fitness before change \",swarms['sub-swarm '+str(i)]['gbest'],swarms['sub-swarm '+str(i)]['gbest-val'])\n",
        "      '''\n",
        "      swarms['sub-swarm '+str(i)]['gbest']=copy.deepcopy(sub_swarm_particles['particle '+str(j)]['cur_pos'])\n",
        "      swarms['sub-swarm '+str(i)]['gbest-val']=d_t\n",
        "      gbest_tracer=d_t\n",
        "      '''print(\"Global best position,fitness after change \",swarms['sub-swarm '+str(i)]['gbest'],swarms['sub-swarm '+str(i)]['gbest-val'])\n",
        "      '''\n",
        "      #print(d_t)\n",
        "      #sub_swarm_particles['particle '+str(j)]['best_pos-val']=***********\n",
        "    else:\n",
        "      if swarms['sub-swarm '+str(i)]['gbest-val']!=gbest_tracer:\n",
        "        swarms['sub-swarm '+str(i)]['gbest']=np.zeros(len(U[i]))\n",
        "        swarms['sub-swarm '+str(i)]['gbest-val']=gbest_tracer\n",
        "    temp_p_f=d_t\n",
        "    # swarms['sub-swarm '+str(i)]['fit-par-t'][SN[i]+p]=temp_p_f\n",
        "    # swarms['sub-swarm '+str(i)]['fit-par-t-1'][SN[i]+p]=temp_p_f\n",
        "    #print(swarms['sub-swarm '+str(i)]['fit-par-t'])\n",
        "    swarms['sub-swarm '+str(i)]['fit-par-t']=np.append(swarms['sub-swarm '+str(i)]['fit-par-t'],temp_p_f)\n",
        "    swarms['sub-swarm '+str(i)]['fit-par-t-1']=np.append(swarms['sub-swarm '+str(i)]['fit-par-t-1'],temp_p_f)\n",
        "    #print(swarms['sub-swarm '+str(i)]['fit-par-t'])\n",
        "    #print(temp_p_f)\n",
        "    #print(\"hello\")\n",
        "  SN[i]=SN[i]+int(delta_N)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgPh0GQvM6PQ"
      },
      "source": [
        "def particle_deletion(swarms,i,SN,delta_N):\n",
        "  chk=0\n",
        "  while True:\n",
        "    #print(chk)\n",
        "    chk=chk+1\n",
        "    if chk>1000:\n",
        "      print(\"feeling drain\")\n",
        "      SN[i]=len(swarms['sub-swarm '+str(i)]['particles'])\n",
        "      return\n",
        "    ele_del=[]\n",
        "    cluster_res=fitness_guided_b_clu_str(swarms,i,SN)\n",
        "    #print(cluster_res)\n",
        "    print(\"\\tcurrent  NO. of Deleted particles : \"+str(int(SN[i]-len(swarms['sub-swarm '+str(i)]['particles']))))\n",
        "    crowd_deg=np.zeros(len(cluster_res))\n",
        "    dc=0\n",
        "    #print(cluster_res)\n",
        "    for k in cluster_res:\n",
        "      t_i=swarms['sub-swarm '+str(i)]['particles']\n",
        "      #print(k[0])\n",
        "      dis=np.abs(t_i['particle '+str(k[0])]['cur_pos']-t_i['particle '+str(k[1])]['cur_pos']).sum()\n",
        "      crowd_deg[dc]=dis\n",
        "      dc=dc+1\n",
        "    min_par_val=min(len(swarms['sub-swarm '+str(i)]['particles'])-(SN[i]-delta_N),len(cluster_res))\n",
        "    F1=crowd_deg.argsort(kind='mergesort')\n",
        "    #print(min_par_val)\n",
        "    tot_len=len(swarms['sub-swarm '+str(i)]['particles'])\n",
        "    for k in range(int(min_par_val)):\n",
        "      #print(k)\n",
        "      # p1_fit=fitness_particle(swarms,i,cluster_res[k][0])\n",
        "      # p2_fit=fitness_particle(swarms,i,cluster_res[k][1])\n",
        "      p1_fit=swarms['sub-swarm '+str(i)]['particles']['particle '+str(cluster_res[k][0])]['best_pos-val']\n",
        "      p2_fit=swarms['sub-swarm '+str(i)]['particles']['particle '+str(cluster_res[k][1])]['best_pos-val']\n",
        "      if p1_fit<p2_fit:\n",
        "        del(swarms['sub-swarm '+str(i)]['particles']['particle '+str(cluster_res[k][0])])\n",
        "        ele_del.append(cluster_res[k][0])\n",
        "      else:\n",
        "        del(swarms['sub-swarm '+str(i)]['particles']['particle '+str(cluster_res[k][1])])\n",
        "        ele_del.append(cluster_res[k][1])\n",
        "    np.delete(swarms['sub-swarm '+str(i)]['fit-par-t'],ele_del)\n",
        "    np.delete(swarms['sub-swarm '+str(i)]['fit-par-t-1'],ele_del)\n",
        "    p=0\n",
        "    s_s_p={}\n",
        "    #print(len(swarms['sub-swarm '+str(i)]['particles']))\n",
        "    for k in range(tot_len):\n",
        "      #print(k)\n",
        "      t_i=swarms['sub-swarm '+str(i)]['particles']\n",
        "      try:\n",
        "        s_s_p['particle '+str(p)]={}\n",
        "        s_s_p['particle '+str(p)]['cur_pos']=t_i['particle '+str(k)]['cur_pos']\n",
        "        #s_s_p['particle '+str(p)]['cur_vel']=t_i['particle '+str(k)]['cur_vel']\n",
        "        s_s_p['particle '+str(p)]['best_pos-val']=t_i['particle '+str(k)]['best_pos-val']\n",
        "        s_s_p['particle '+str(p)]['best_pos']=t_i['particle '+str(k)]['best_pos']\n",
        "        p=p+1\n",
        "      except:\n",
        "        print(\"\\te7\")\n",
        "        del(s_s_p['particle '+str(p)])\n",
        "        pass\n",
        "    swarms['sub-swarm '+str(i)]['particles']=s_s_p\n",
        "    #print(swarms['sub-swarm '+str(i)]['particles']['particle '+str(0)]['cur_pos'])\n",
        "    #print(len(swarms['sub-swarm '+str(i)]['particles']),SN[i]-delta_N)\n",
        "    #print(len(swarms['sub-swarm '+str(i)]['particles']),int(SN[i]-abs(delta_N)))\n",
        "    #if len(swarms['sub-swarm '+str(i)]['particles'])==int(SN[i]-abs(delta_N)):\n",
        "    if int(SN[i]-len(swarms['sub-swarm '+str(i)]['particles']))==int(abs(delta_N)):\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t'],ele_del)\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t-1'],ele_del)\n",
        "      print(\"\\tcurrent  NO. of Deleted particles : \"+str(int(SN[i]-len(swarms['sub-swarm '+str(i)]['particles']))))\n",
        "      SN[i]=len(swarms['sub-swarm '+str(i)]['particles'])\n",
        "      return \n",
        "    else:\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t'],ele_del)\n",
        "      # np.delete(swarms['sub-swarm '+str(i)]['fit-par-t-1'],ele_del)\n",
        "      # if int(SN[i]-len(swarms['sub-swarm '+str(i)]['particles']))==int(abs(delta_N)):\n",
        "      #   SN[i]=len(swarms['sub-swarm '+str(i)]['particles'])\n",
        "      #   return\n",
        "      if len(swarms['sub-swarm '+str(i)]['particles'])<SN[i]-delta_N:\n",
        "        print(\"bug\")\n",
        "        return\n",
        "      if len(swarms['sub-swarm '+str(i)]['particles'])==2:\n",
        "        SN[i]=2\n",
        "        return\n",
        "    # else:\n",
        "    #   print(swarms['sub-swarm '+str(i)]['particles'])\n",
        "    #   return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eipNOnHsS763"
      },
      "source": [
        "def add_result_to_csv(score,fn,nsf):\n",
        "  import csv\n",
        "  fieldnames = ['Iteration','M','# of particles','Total # of features','# of features selected','Running Time of FS','Fit-time','Score-time','Train_Accuracy','Test_Accuracy','Train_precision','Test_precision','Train_recall','Test_recall','Train_F-measure','Test_F-measure','Train_AUC','Test_AUC','gbest']\n",
        "  file_exists = os.path.isfile(('%s.csv')%(fn))\n",
        "  with open(('%s.csv')%(fn), 'a', newline='') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      if not file_exists:\n",
        "        writer.writeheader() \n",
        "      for i in score.keys():\n",
        "        score[i]=np.mean(score[i])\n",
        "      try:\n",
        "        writer.writerow({'Iteration':c, 'M':M,'# of particles':np.sum(SN),'Total # of features':n_s_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro'],'Train_AUC':score['train_roc_auc_ovr'],'Test_AUC':score['test_roc_auc_ovr'],'gbest':gbest_tracer})\n",
        "      except:\n",
        "        writer.writerow({'Iteration':c, 'M':M,'# of particles':np.sum(SN),'Total # of features':n_s_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro'],'gbest':gbest_tracer})\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}