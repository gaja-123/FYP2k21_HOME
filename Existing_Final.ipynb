{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Existing_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qTgwEZB6LYMa",
        "kZ1igBz0LeXd",
        "WizXWLqfjqqE",
        "0S-FnhbvjuWW",
        "x8-uiuLxms0Z",
        "lKIOyJPhpCEB",
        "sS2ZerQaqP9E",
        "sl-MdPDXsGV-",
        "9mGz0pFes762",
        "bOWoAsoYtyxm",
        "4TQRMZtQwYq_",
        "I5yP1fom10l2",
        "wIfGvOMj3Q2r",
        "tLvUKGNzrAv_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "607yMt_Zr_ko"
      },
      "source": [
        "## 11 Existing Algorithms for Comparison\n",
        "# with KNN LOOCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSN0aRwbA9Vg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d753b5-1c3b-43c4-951a-4290422ee39f"
      },
      "source": [
        "!git clone https://github.com/JingweiToo/Wrapper-Feature-Selection-Toolbox-Python.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Wrapper-Feature-Selection-Toolbox-Python'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 163 (delta 38), reused 0 (delta 0), pack-reused 102\u001b[K\n",
            "Receiving objects: 100% (163/163), 81.66 KiB | 4.30 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfs9m0KFBgZs",
        "outputId": "f6bbe0d0-58cd-400d-b29e-285086b67016"
      },
      "source": [
        "cd Wrapper-Feature-Selection-Toolbox-Python/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Wrapper-Feature-Selection-Toolbox-Python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUgKvWA2BlB6"
      },
      "source": [
        "# ls"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCcJ1ZcvBk-c"
      },
      "source": [
        "# cd /content/drive/My Drive/FYP/Wrapper-Feature-Selection-Toolbox-Python-main (copy)/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI436gXJ6vWE",
        "outputId": "61d05884-1d34-4924-dce9-991699aba340"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTgwEZB6LYMa"
      },
      "source": [
        "## down"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtBgD-OWA--1"
      },
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import matplotlib.pyplot as plt\n",
        "# import os\n",
        "\n",
        "# # load data\n",
        "# ds_name=''\n",
        "# data  = pd.read_csv(('/content/drive/My Drive/FYP/Dataset/%s.csv')%(ds_name))\n",
        "# df=data\n",
        "# data  = data.values\n",
        "# feat  = np.asarray(data[:, 0:-1])   # feature vector\n",
        "# label = np.asarray(data[:, -1])     # label vector\n",
        "# n_samples,n_features=data.shape\n",
        "# # split data into train & validation (70 -- 30)\n",
        "# xtrain, xtest, ytrain, ytest = train_test_split(feat, label, test_size=0.3)\n",
        "# fold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}\n",
        "\n",
        "\n",
        "# ## creating directories\n",
        "\n",
        "# if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\n",
        "#   os.makedirs(('/content/drive/My Drive/FYP/Existing'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Particle Swarm Optimization ( PSO )\n",
        "# c=None\n",
        "# rt=None\n",
        "# algo_name='PSO'\n",
        "# if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\n",
        "#   os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\n",
        "\n",
        "# hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\n",
        "# if not os.path.exists(('%s/%s')%(hp,ds_name)):\n",
        "#     reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\n",
        "#     results_csv=(('%s/%s/results_csv')%(hp,ds_name))\n",
        "#     temp_storage=('%s/%s/temp')%(hp,ds_name)\n",
        "#     results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\n",
        "#     os.makedirs(('%s/%s')%(hp,ds_name))\n",
        "#     os.makedirs(reduced_datasets)\n",
        "#     os.makedirs(results_csv)\n",
        "#     os.makedirs(results_graphs)\n",
        "#     os.makedirs(temp_storage)\n",
        "  \n",
        "# else:\n",
        "#   print(\"Error Already File Exists!!!!\")\n",
        "#   reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\n",
        "#   results_csv=(('%s/%s/results_csv')%(hp,ds_name))\n",
        "#   temp_storage=('%s/%s/temp')%(hp,ds_name)\n",
        "#   results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\n",
        "#   import sys\n",
        "#   #sys.exit(\"Error message\")\n",
        "\n",
        "\n",
        "\n",
        "# # from FS.pso import jfs \n",
        "# # from FS.utility import *\n",
        "# # parameter\n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of particles\n",
        "# T    = 100   # maximum number of iterations\n",
        "# w    = 0.9\n",
        "# c1   = 2\n",
        "# c2   = 2\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'w':w, 'c1':c1, 'c2':c2}\n",
        "\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# # sf   = fmdl['sf']\n",
        "\n",
        "# # # number of selected features\n",
        "# # num_feat = fmdl['nf']\n",
        "# # print(\"Feature Size:\", num_feat)\n",
        "# # #print(sf.shape)\n",
        "# # sf=np.append(sf,n_features-1)\n",
        "# # #sf=sf.reshape(1,len(sf))\n",
        "# # #print(sf.shape)\n",
        "# # df=data[:,sf]\n",
        "# # df= pd.DataFrame(data=df)\n",
        "# # nb_tree_svm(df)\n",
        "\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Genetic Algorithm ( GA )\n",
        "\n",
        "# from FS.ga import jfs \n",
        "# # parameter\n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# CR   = 0.8\n",
        "# MR   = 0.01\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'CR':CR, 'MR':MR}\n",
        "\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Firefly Algorithm (FA)\n",
        "\n",
        "# from FS.fa import jfs \n",
        "# # parameter\n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# alpha  = 1       # constant\n",
        "# beta0  = 1       # light amplitude\n",
        "# gamma  = 1       # absorbtion coefficient\n",
        "# theta  = 0.97    # control alpha\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'alpha':alpha, 'beta0':beta0, 'gamma':gamma, 'theta':theta}\n",
        "\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Cuckoo Search (CS)\n",
        "# from FS.cs import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# Pa  = 0.25   # discovery rate\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'Pa':Pa}\n",
        "\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##  Differential Evolution (DE)\n",
        "# #from FS.de import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# CR = 0.9    # crossover rate\n",
        "# F  = 0.5    # constant factor\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'CR':CR, 'F':F}\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Whale Optimization Algorithm (WOA)\n",
        "\n",
        "# from FS.woa import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# b  = 1    # constant\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'b':b}\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Bat Algorithm (BA)\n",
        "# from FS.ba import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# fmax   = 2      # maximum frequency\n",
        "# fmin   = 0      # minimum frequency\n",
        "# alpha  = 0.9    # constant\n",
        "# gamma  = 0.9    # constant\n",
        "# A      = 2      # maximum loudness\n",
        "# r      = 1      # maximum pulse rate\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'fmax':fmax, 'fmin':fmin, 'alpha':alpha, 'gamma':gamma, 'A':A, 'r':r}\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Flower Pollination Algorithm (FPA)\n",
        "# from FS.fpa import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# P  = 0.8      # switch probability\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'P':P}\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Grey Wolf Optimizer\t\n",
        "\n",
        "\n",
        "# from FS.gwo import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T}\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Harris Hawk Optimization\t\n",
        "\n",
        "# from FS.hho import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T}\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')\n",
        "\n",
        "\n",
        "# ## Salp Swarm Algorithm\t\n",
        "# from FS.ssa import jfs \n",
        "# k    = 5     # k-value in KNN\n",
        "# N    = 10    # number of chromosomes\n",
        "# T    = 100   # maximum number of generations\n",
        "# opts = {'k':k, 'fold':fold, 'N':N, 'T':T}\n",
        "# # perform feature selection\n",
        "# fmdl = jfs(feat, label, opts)\n",
        "# sf   = fmdl['sf']\n",
        "\n",
        "# # number of selected features\n",
        "# num_feat = fmdl['nf']\n",
        "# print(\"Feature Size:\", num_feat)\n",
        "# #print(sf.shape)\n",
        "# sf=np.append(sf,n_features-1)\n",
        "# #sf=sf.reshape(1,len(sf))\n",
        "# #print(sf.shape)\n",
        "# df=data[:,sf]\n",
        "# df= pd.DataFrame(data=df)\n",
        "# nb_tree_svm(df)\n",
        "\n",
        "# plot_pdf('naive bayes')\n",
        "# plot_pdf('tree')\n",
        "# plot_pdf('svm_linear')\n",
        "# plot_pdf('knn')\n",
        "\n",
        "# print(\"For Naive Bayes\")\n",
        "# mean_clf('naive bayes')\n",
        "# print(\"\\nFor Tree\")\n",
        "# mean_clf('tree')\n",
        "# print(\"\\nFor SVM Linear\")\n",
        "# mean_clf('svm_linear')\n",
        "# print(\"\\nFor KNN\")\n",
        "# mean_clf('knn')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ1igBz0LeXd"
      },
      "source": [
        "## up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8w_eizsEfCA"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "####changed to KNN-LOOCV######\n",
        "# error rate\n",
        "def error_rate(xtrain, ytrain, x, opts):\n",
        "    from sklearn.model_selection import LeaveOneOut\n",
        "    # parameters\n",
        "    k     = opts['k']\n",
        "    fold  = opts['fold']\n",
        "    colnames=fold.columns\n",
        "    temp=fold.loc[:, colnames !=colnames[len(colnames)-1]]\n",
        "    temp=temp.values\n",
        "    X=temp[:, x == 1]\n",
        "    y=fold.iloc[:,-1:]\n",
        "    #print(y)\n",
        "    acc=knn(X,y)\n",
        "    error   = 1 - acc\n",
        "    \n",
        "    return error\n",
        "\n",
        "\n",
        "# Error rate & Feature size\n",
        "def Fun(xtrain, ytrain, x, opts):\n",
        "    # Parameters\n",
        "    alpha    = 0.99\n",
        "    beta     = 1 - alpha\n",
        "    # Original feature size\n",
        "    max_feat = len(x)\n",
        "    # Number of selected features\n",
        "    num_feat = np.sum(x == 1)\n",
        "    # Solve if no feature selected\n",
        "    if num_feat == 0:\n",
        "        cost  = 1\n",
        "    else:\n",
        "        # Get error rate\n",
        "        error = error_rate(xtrain, ytrain, x, opts)\n",
        "        # Objective function\n",
        "        cost  = alpha * error + beta * (num_feat / max_feat)\n",
        "        \n",
        "    return cost\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WizXWLqfjqqE"
      },
      "source": [
        "## PSO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFbLUhuLEHTQ"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "##added\n",
        "#from FS.utility import *\n",
        "import time\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def init_velocity(lb, ub, N, dim):\n",
        "    V    = np.zeros([N, dim], dtype='float')\n",
        "    Vmax = np.zeros([1, dim], dtype='float')\n",
        "    Vmin = np.zeros([1, dim], dtype='float')\n",
        "    # Maximum & minimum velocity\n",
        "    for d in range(dim):\n",
        "        Vmax[0,d] = (ub[0,d] - lb[0,d]) / 2\n",
        "        Vmin[0,d] = -Vmax[0,d]\n",
        "        \n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            V[i,d] = Vmin[0,d] + (Vmax[0,d] - Vmin[0,d]) * rand()\n",
        "        \n",
        "    return V, Vmax, Vmin\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "def PSO(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub    = 1\n",
        "    lb    = 0\n",
        "    thres = 0.5\n",
        "    w     = 0.9    # inertia weight\n",
        "    c1    = 2      # acceleration factor\n",
        "    c2    = 2      # acceleration factor\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    if 'w' in opts:\n",
        "        w    = opts['w']\n",
        "    if 'c1' in opts:\n",
        "        c1   = opts['c1']\n",
        "    if 'c2' in opts:\n",
        "        c2   = opts['c2'] \n",
        "    \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position & velocity\n",
        "    X             = init_position(lb, ub, N, dim)\n",
        "    V, Vmax, Vmin = init_velocity(lb, ub, N, dim) \n",
        "    \n",
        "    # Pre\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xgb   = np.zeros([1, dim], dtype='float')\n",
        "    fitG  = float('inf')\n",
        "    Xpb   = np.zeros([N, dim], dtype='float')\n",
        "    fitP  = float('inf') * np.ones([N, 1], dtype='float')\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "   \n",
        "    while t < max_iter:\n",
        "        #added\n",
        "        c=t\n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(X, thres, N, dim)\n",
        "        \n",
        "        # Fitness\n",
        "        for i in range(N):\n",
        "            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if fit[i,0] < fitP[i,0]:\n",
        "                Xpb[i,:]  = X[i,:]\n",
        "                fitP[i,0] = fit[i,0]\n",
        "            if fitP[i,0] < fitG:\n",
        "                Xgb[0,:]  = Xpb[i,:]\n",
        "                fitG      = fitP[i,0]\n",
        "        \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Iteration:\", t + 1)\n",
        "        print(\"Best (PSO):\", curve[0,t])\n",
        "        t += 1\n",
        "        \n",
        "        for i in range(N):\n",
        "            for d in range(dim):\n",
        "                # Update velocity\n",
        "                r1     = rand()\n",
        "                r2     = rand()\n",
        "                V[i,d] = w * V[i,d] + c1 * r1 * (Xpb[i,d] - X[i,d]) + c2 * r2 * (Xgb[0,d] - X[i,d]) \n",
        "                # Boundary\n",
        "                V[i,d] = boundary(V[i,d], Vmin[0,d], Vmax[0,d])\n",
        "                # Update position\n",
        "                X[i,d] = X[i,d] + V[i,d]\n",
        "                # Boundary\n",
        "                X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "       \n",
        "    # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # pso_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    #return pso_data    \n",
        "    return\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S-FnhbvjuWW"
      },
      "source": [
        "## GA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pc-ER-zjoNg"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def roulette_wheel(prob):\n",
        "    num = len(prob)\n",
        "    C   = np.cumsum(prob)\n",
        "    P   = rand()\n",
        "    for i in range(num):\n",
        "        if C[i] > P:\n",
        "            index = i;\n",
        "            break\n",
        "    \n",
        "    return index\n",
        "\n",
        "\n",
        "def GA(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub       = 1\n",
        "    lb       = 0\n",
        "    thres    = 0.5    \n",
        "    CR       = 0.8     # crossover rate\n",
        "    MR       = 0.01    # mutation rate\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    if 'CR' in opts:\n",
        "        CR   = opts['CR'] \n",
        "    if 'MR' in opts: \n",
        "        MR   = opts['MR']  \n",
        " \n",
        "     # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Binary conversion\n",
        "    X     = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xgb   = np.zeros([1, dim], dtype='int')\n",
        "    fitG  = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, X[i,:], opts)\n",
        "        if fit[i,0] < fitG:\n",
        "            Xgb[0,:] = X[i,:]\n",
        "            fitG     = fit[i,0]\n",
        "    \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float')\n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = fitG.copy()\n",
        "    print(\"Generation:\", t + 1)\n",
        "    print(\"Best (GA):\", curve[0,t])\n",
        "    t += 1\n",
        "    \n",
        "    while t < max_iter:\n",
        "        ##added\n",
        "        c=t\n",
        "        # Probability\n",
        "        inv_fit = 1 / (1 + fit)\n",
        "        prob    = inv_fit / np.sum(inv_fit) \n",
        " \n",
        "        # Number of crossovers\n",
        "        Nc = 0\n",
        "        for i in range(N):\n",
        "            if rand() < CR:\n",
        "              Nc += 1\n",
        "              \n",
        "        x1 = np.zeros([Nc, dim], dtype='int')\n",
        "        x2 = np.zeros([Nc, dim], dtype='int')\n",
        "        for i in range(Nc):\n",
        "            # Parent selection\n",
        "            k1      = roulette_wheel(prob)\n",
        "            k2      = roulette_wheel(prob)\n",
        "            P1      = X[k1,:].copy()\n",
        "            P2      = X[k2,:].copy()\n",
        "            # Random one dimension from 1 to dim\n",
        "            index   = np.random.randint(low = 1, high = dim-1)\n",
        "            # Crossover\n",
        "            x1[i,:] = np.concatenate((P1[0:index] , P2[index:]))\n",
        "            x2[i,:] = np.concatenate((P2[0:index] , P1[index:]))\n",
        "            # Mutation\n",
        "            for d in range(dim):\n",
        "                if rand() < MR:\n",
        "                    x1[i,d] = 1 - x1[i,d]\n",
        "                    \n",
        "                if rand() < MR:\n",
        "                    x2[i,d] = 1 - x2[i,d]\n",
        "\n",
        "        \n",
        "        # Merge two group into one\n",
        "        Xnew = np.concatenate((x1 , x2), axis=0)\n",
        "        \n",
        "        # Fitness\n",
        "        Fnew = np.zeros([2 * Nc, 1], dtype='float')\n",
        "        for i in range(2 * Nc):\n",
        "            Fnew[i,0] = Fun(xtrain, ytrain, Xnew[i,:], opts)\n",
        "            if Fnew[i,0] < fitG:\n",
        "                Xgb[0,:] = Xnew[i,:]\n",
        "                fitG     = Fnew[i,0]\n",
        "                   \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Generation:\", t + 1)\n",
        "        print(\"Best (GA):\", curve[0,t])\n",
        "        t += 1\n",
        "        \n",
        "        # Elitism \n",
        "        XX  = np.concatenate((X , Xnew), axis=0)\n",
        "        FF  = np.concatenate((fit , Fnew), axis=0)\n",
        "        # Sort in ascending order\n",
        "        ind = np.argsort(FF, axis=0)\n",
        "        for i in range(N):\n",
        "            X[i,:]   = XX[ind[i,0],:]\n",
        "            fit[i,0] = FF[ind[i,0]]\n",
        "        \n",
        "        if True:\n",
        "            Gbin       = Xgb[0,:]\n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "            \n",
        "    # Best feature subset\n",
        "    # Gbin       = Xgb[0,:]\n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # ga_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return ga_data \n",
        "    return\n",
        "            \n",
        "            \n",
        "                \n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8-uiuLxms0Z"
      },
      "source": [
        "## FA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4-A1tzbmshB"
      },
      "source": [
        "#[2010]-\"Firefly algorithm,stochastic test functions and design optimization\" \n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def FA(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub     = 1\n",
        "    lb     = 0\n",
        "    thres  = 0.5\n",
        "    alpha  = 1       # constant\n",
        "    beta0  = 1       # light amplitude\n",
        "    gamma  = 1       # absorbtion coefficient\n",
        "    theta  = 0.97    # control alpha\n",
        "    \n",
        "    N          = opts['N']\n",
        "    max_iter   = opts['T']\n",
        "    if 'alpha' in opts:\n",
        "        alpha  = opts['alpha'] \n",
        "    if 'beta0' in opts:\n",
        "        beta0  = opts['beta0'] \n",
        "    if 'gamma' in opts:\n",
        "        gamma  = opts['gamma'] \n",
        "    if 'theta' in opts:\n",
        "        theta  = opts['theta'] \n",
        "        \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Binary conversion\n",
        "    Xbin  = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xgb   = np.zeros([1, dim], dtype='float')\n",
        "    fitG  = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "        if fit[i,0] < fitG:\n",
        "            Xgb[0,:] = X[i,:]\n",
        "            fitG     = fit[i,0]\n",
        "    \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = fitG.copy()\n",
        "    print(\"Generation:\", t + 1)\n",
        "    print(\"Best (FA):\", curve[0,t])\n",
        "    t += 1\n",
        "        \n",
        "    while t <max_iter:  \n",
        "        ##added\n",
        "        c=t\n",
        "        # Alpha update\n",
        "        alpha = alpha * theta\n",
        "        # Rank firefly based on their light intensity\n",
        "        ind   = np.argsort(fit, axis=0)\n",
        "        FF    = fit.copy()\n",
        "        XX    = X.copy()\n",
        "        for i in range(N):\n",
        "            fit[i,0] = FF[ind[i,0]]\n",
        "            X[i,:]   = XX[ind[i,0],:]\n",
        "        \n",
        "        for i in range(N):\n",
        "            # The attractiveness parameter\n",
        "            for j in range(N):\n",
        "                # Update moves if firefly j brighter than firefly i\n",
        "                if fit[i,0] > fit[j,0]: \n",
        "                    # Compute Euclidean distance \n",
        "                    r    = np.sqrt(np.sum((X[i,:] - X[j,:]) ** 2))\n",
        "                    # Beta (2)\n",
        "                    beta = beta0 * np.exp(-gamma * r ** 2)\n",
        "                    for d in range(dim):\n",
        "                        # Update position (3)\n",
        "                        eps    = rand() - 0.5\n",
        "                        X[i,d] = X[i,d] + beta * (X[j,d] - X[i,d]) + alpha * eps \n",
        "                        # Boundary\n",
        "                        X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "\n",
        "                    # Binary conversion\n",
        "                    temp      = np.zeros([1, dim], dtype='float')\n",
        "                    temp[0,:] = X[i,:]  \n",
        "                    Xbin      = binary_conversion(temp, thres, 1, dim)\n",
        "                    \n",
        "                    # fitness\n",
        "                    fit[i,0]  = Fun(xtrain, ytrain, Xbin[0,:], opts)\n",
        "                    \n",
        "                    # best update        \n",
        "                    if fit[i,0] < fitG:\n",
        "                        Xgb[0,:] = X[i,:]\n",
        "                        fitG     = fit[i,0]\n",
        "                \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Generation:\", t + 1)\n",
        "        print(\"Best (FA):\", curve[0,t])\n",
        "        t += 1\n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()            \n",
        "\n",
        "            \n",
        "    # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # fa_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return fa_data  \n",
        "    return\n",
        "\n",
        "\n",
        "            \n",
        "            \n",
        "                "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKIOyJPhpCEB"
      },
      "source": [
        "## CS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcnDBhw0pjlU"
      },
      "source": [
        "#[2009]-\"Cuckoo search via Levy flights\" \n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "import math\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "# Levy Flight\n",
        "def levy_distribution(beta, dim):\n",
        "    # Sigma     \n",
        "    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n",
        "    deno  = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)\n",
        "    sigma = (nume / deno) ** (1 / beta) \n",
        "    # Parameter u & v \n",
        "    u     = np.random.randn(dim) * sigma\n",
        "    v     = np.random.randn(dim)\n",
        "    # Step \n",
        "    step  = u / abs(v) ** (1 / beta)\n",
        "    LF    = 0.01 * step\n",
        "    \n",
        "    return LF\n",
        "\n",
        "\n",
        "def CS(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub     = 1\n",
        "    lb     = 0\n",
        "    thres  = 0.5\n",
        "    Pa     = 0.25     # discovery rate\n",
        "    alpha  = 1        # constant\n",
        "    beta   = 1.5      # levy component\n",
        "    \n",
        "    N          = opts['N']\n",
        "    max_iter   = opts['T']\n",
        "    if 'Pa' in opts:\n",
        "        Pa   = opts['Pa'] \n",
        "    if 'alpha' in opts:\n",
        "        alpha   = opts['alpha'] \n",
        "    if 'beta' in opts:\n",
        "        beta  = opts['beta'] \n",
        "        \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Binary conversion\n",
        "    Xbin  = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xgb   = np.zeros([1, dim], dtype='float')\n",
        "    fitG  = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "        if fit[i,0] < fitG:\n",
        "            Xgb[0,:] = X[i,:]\n",
        "            fitG     = fit[i,0]\n",
        "    \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = fitG.copy()\n",
        "    print(\"Generation:\", t + 1)\n",
        "    print(\"Best (CS):\", curve[0,t])\n",
        "    t += 1\n",
        "        \n",
        "    while t <max_iter:  \n",
        "        ##added\n",
        "        c=t\n",
        "        Xnew  = np.zeros([N, dim], dtype='float') \n",
        "        \n",
        "        # {1} Random walk/Levy flight phase\n",
        "        for i in range(N):\n",
        "            # Levy distribution\n",
        "            L = levy_distribution(beta,dim)\n",
        "            for d in range(dim):\n",
        "                # Levy flight (1)\n",
        "                Xnew[i,d] = X[i,d] + alpha * L[d] * (X[i,d] - Xgb[0,d]) \n",
        "                # Boundary\n",
        "                Xnew[i,d] = boundary(Xnew[i,d], lb[0,d], ub[0,d])\n",
        "      \n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(Xnew, thres, N, dim)\n",
        "        \n",
        "        # Greedy selection\n",
        "        for i in range(N):\n",
        "            Fnew = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if Fnew <= fit[i,0]:\n",
        "                X[i,:]   = Xnew[i,:]\n",
        "                fit[i,0] = Fnew             \n",
        "                \n",
        "            if fit[i,0] < fitG:\n",
        "                Xgb[0,:] = X[i,:]\n",
        "                fitG     = fit[i,0]\n",
        "        \n",
        "        # {2} Discovery and abandon worse nests phase\n",
        "        J  = np.random.permutation(N)\n",
        "        K  = np.random.permutation(N)\n",
        "        Xj = np.zeros([N, dim], dtype='float')\n",
        "        Xk = np.zeros([N, dim], dtype='float')\n",
        "        for i in range(N):\n",
        "            Xj[i,:] = X[J[i],:]\n",
        "            Xk[i,:] = X[K[i],:]\n",
        "        \n",
        "        Xnew  = np.zeros([N, dim], dtype='float') \n",
        "        \n",
        "        for i in range(N): \n",
        "            Xnew[i,:] = X[i,:]\n",
        "            r         = rand()\n",
        "            for d in range(dim):\n",
        "                # A fraction of worse nest is discovered with a probability\n",
        "                if rand() < Pa:\n",
        "                    Xnew[i,d] = X[i,d] + r * (Xj[i,d] - Xk[i,d])\n",
        "                \n",
        "                # Boundary\n",
        "                Xnew[i,d] = boundary(Xnew[i,d], lb[0,d], ub[0,d])\n",
        "        \n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(Xnew, thres, N, dim)\n",
        "        \n",
        "        # Greedy selection\n",
        "        for i in range(N):\n",
        "            Fnew = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if Fnew <= fit[i,0]:\n",
        "                X[i,:]   = Xnew[i,:]\n",
        "                fit[i,0] = Fnew             \n",
        "                \n",
        "            if fit[i,0] < fitG:\n",
        "                Xgb[0,:] = X[i,:]\n",
        "                fitG     = fit[i,0]\n",
        "                \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Generation:\", t + 1)\n",
        "        print(\"Best (CS):\", curve[0,t])\n",
        "        t += 1            \n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "            \n",
        "    # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # cs_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return cs_data  \n",
        "    return\n",
        "\n",
        "            \n",
        "            \n",
        "                "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS2ZerQaqP9E"
      },
      "source": [
        "## DE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ew9CvK8rAU8"
      },
      "source": [
        "#[1997]-\"Differential evolution - A simple and efficient heuristic for global optimization over continuous spaces\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "def DE(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub    = 1\n",
        "    lb    = 0\n",
        "    thres = 0.5\n",
        "    CR    = 0.9     # crossover rate\n",
        "    F     = 0.5     # factor\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    if 'CR' in opts:\n",
        "        CR   = opts['CR'] \n",
        "    if 'F' in opts:\n",
        "        F    = opts['F']     \n",
        "    \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Binary conversion\n",
        "    Xbin  = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xgb   = np.zeros([1, dim], dtype='float')\n",
        "    fitG  = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "        if fit[i,0] < fitG:\n",
        "            Xgb[0,:] = X[i,:]\n",
        "            fitG     = fit[i,0]\n",
        "    \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = fitG.copy()\n",
        "    print(\"Generation:\", t + 1)\n",
        "    print(\"Best (DE):\", curve[0,t])\n",
        "    t += 1\n",
        "\n",
        "    while t < max_iter:\n",
        "        ##added\n",
        "        c=t  \n",
        "        V = np.zeros([N, dim], dtype='float')\n",
        "        U = np.zeros([N, dim], dtype='float')\n",
        "        \n",
        "        for i in range(N):\n",
        "            # Choose r1, r2, r3 randomly, but not equal to i \n",
        "            RN = np.random.permutation(N)\n",
        "            for j in range(N):\n",
        "                if RN[j] == i:\n",
        "                    RN = np.delete(RN, j)\n",
        "                    break\n",
        "                \n",
        "            r1 = RN[0]\n",
        "            r2 = RN[1]\n",
        "            r3 = RN[2]\n",
        "            # mutation (2)\n",
        "            for d in range(dim):\n",
        "                V[i,d] = X[r1,d] + F * (X[r2,d] - X[r3,d])\n",
        "                # Boundary\n",
        "                V[i,d] = boundary(V[i,d], lb[0,d], ub[0,d])\n",
        "            \n",
        "            # Random one dimension from 1 to dim\n",
        "            index = np.random.randint(low = 0, high = dim)\n",
        "            # crossover (3-4)\n",
        "            for d in range(dim):\n",
        "                if (rand() <= CR)  or  (d == index):\n",
        "                    U[i,d] = V[i,d]\n",
        "                else:\n",
        "                    U[i,d] = X[i,d]\n",
        "        \n",
        "        # Binary conversion\n",
        "        Ubin = binary_conversion(U, thres, N, dim)\n",
        "        \n",
        "        # Selection\n",
        "        for i in range(N):\n",
        "            fitU = Fun(xtrain, ytrain, Ubin[i,:], opts)\n",
        "            if fitU <= fit[i,0]:\n",
        "                X[i,:]   = U[i,:]\n",
        "                fit[i,0] = fitU\n",
        "                \n",
        "            if fit[i,0] < fitG:\n",
        "                Xgb[0,:] = X[i,:]\n",
        "                fitG     = fit[i,0]\n",
        "            \n",
        "                \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Generation:\", t + 1)\n",
        "        print(\"Best (DE):\", curve[0,t])\n",
        "        t += 1            \n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "            \n",
        "    # # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # de_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return de_data  \n",
        "    return\n",
        "\n",
        "            \n",
        "            \n",
        "                "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl-MdPDXsGV-"
      },
      "source": [
        "## WOA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGOy7ZOLsIMd"
      },
      "source": [
        "#[2016]-\"The whale optimization algorithm\"]\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def WOA(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub    = 1\n",
        "    lb    = 0\n",
        "    thres = 0.5\n",
        "    b     = 1       # constant\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    if 'b' in opts:\n",
        "        b    = opts['b']\n",
        "    \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X    = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Binary conversion\n",
        "    Xbin = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit  = np.zeros([N, 1], dtype='float')\n",
        "    Xgb  = np.zeros([1, dim], dtype='float')\n",
        "    fitG = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "        if fit[i,0] < fitG:\n",
        "            Xgb[0,:] = X[i,:]\n",
        "            fitG     = fit[i,0]\n",
        "        \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = fitG.copy()\n",
        "    print(\"Generation:\", t + 1)\n",
        "    print(\"Best (WOA):\", curve[0,t])\n",
        "    t += 1\n",
        "\n",
        "    while t < max_iter:\n",
        "        ##added\n",
        "        c=t  \n",
        "        # Define a, linearly decreases from 2 to 0 \n",
        "        a = 2 - t * (2 / max_iter)\n",
        "        \n",
        "        for i in range(N):\n",
        "            # Parameter A (2.3)\n",
        "            A = 2 * a * rand() - a\n",
        "            # Paramater C (2.4)\n",
        "            C = 2 * rand()\n",
        "            # Parameter p, random number in [0,1]\n",
        "            p = rand()\n",
        "            # Parameter l, random number in [-1,1]\n",
        "            l = -1 + 2 * rand()  \n",
        "            # Whale position update (2.6)\n",
        "            if p  < 0.5:\n",
        "                # {1} Encircling prey\n",
        "                if abs(A) < 1:\n",
        "                    for d in range(dim):\n",
        "                        # Compute D (2.1)\n",
        "                        Dx     = abs(C * Xgb[0,d] - X[i,d])\n",
        "                        # Position update (2.2)\n",
        "                        X[i,d] = Xgb[0,d] - A * Dx\n",
        "                        # Boundary\n",
        "                        X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "                \n",
        "                # {2} Search for prey\n",
        "                elif abs(A) >= 1:\n",
        "                    for d in range(dim):\n",
        "                        # Select a random whale\n",
        "                        k      = np.random.randint(low = 0, high = N)\n",
        "                        # Compute D (2.7)\n",
        "                        Dx     = abs(C * X[k,d] - X[i,d])\n",
        "                        # Position update (2.8)\n",
        "                        X[i,d] = X[k,d] - A * Dx\n",
        "                        # Boundary\n",
        "                        X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "            \n",
        "            # {3} Bubble-net attacking \n",
        "            elif p >= 0.5:\n",
        "                for d in range(dim):\n",
        "                    # Distance of whale to prey\n",
        "                    dist   = abs(Xgb[0,d] - X[i,d])\n",
        "                    # Position update (2.5)\n",
        "                    X[i,d] = dist * np.exp(b * l) * np.cos(2 * np.pi * l) + Xgb[0,d] \n",
        "                    # Boundary\n",
        "                    X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "        \n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(X, thres, N, dim)\n",
        "        \n",
        "        # Fitness\n",
        "        for i in range(N):\n",
        "            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if fit[i,0] < fitG:\n",
        "                Xgb[0,:] = X[i,:]\n",
        "                fitG     = fit[i,0]\n",
        "        \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Generation:\", t + 1)\n",
        "        print(\"Best (WOA):\", curve[0,t])\n",
        "        t += 1            \n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "            \n",
        "    # # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)    \n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # woa_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return woa_data \n",
        "    return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mGz0pFes762"
      },
      "source": [
        "## BA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8NgWlpQs91R"
      },
      "source": [
        "#[2010]-\"A new metaheuristic bat-inspired algorithm\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "def BA(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub     = 1\n",
        "    lb     = 0\n",
        "    thres  = 0.5\n",
        "    fmax   = 2      # maximum frequency\n",
        "    fmin   = 0      # minimum frequency\n",
        "    alpha  = 0.9    # constant\n",
        "    gamma  = 0.9    # constant\n",
        "    A_max  = 2      # maximum loudness\n",
        "    r0_max = 1      # maximum pulse rate\n",
        "    \n",
        "    N          = opts['N']\n",
        "    max_iter   = opts['T']\n",
        "    if 'fmax' in opts:\n",
        "        fmax   = opts['fmax'] \n",
        "    if 'fmin' in opts:\n",
        "        fmin   = opts['fmin'] \n",
        "    if 'alpha' in opts:\n",
        "        alpha  = opts['alpha'] \n",
        "    if 'gamma' in opts:\n",
        "        gamma  = opts['gamma'] \n",
        "    if 'A' in opts:\n",
        "        A_max  = opts['A'] \n",
        "    if 'r' in opts:\n",
        "        r0_max = opts['r'] \n",
        "        \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position & velocity\n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    V     = np.zeros([N, dim], dtype='float')\n",
        "    \n",
        "    # Binary conversion\n",
        "    Xbin  = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xgb   = np.zeros([1, dim], dtype='float')\n",
        "    fitG  = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "        if fit[i,0] < fitG:\n",
        "            Xgb[0,:] = X[i,:]\n",
        "            fitG     = fit[i,0]\n",
        "    \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = fitG.copy()\n",
        "    print(\"Generation:\", t + 1)\n",
        "    print(\"Best (BA):\", curve[0,t])\n",
        "    t += 1\n",
        "    \n",
        "    # Initial loudness [1 ~ 2] & pulse rate [0 ~ 1]\n",
        "    A  = np.random.uniform(1, A_max, N)\n",
        "    r0 = np.random.uniform(0, r0_max, N)\n",
        "    r  = r0.copy()\n",
        "    \n",
        "    while t < max_iter:  \n",
        "        ##added\n",
        "        c=t  \n",
        "        Xnew  = np.zeros([N, dim], dtype='float') \n",
        "        \n",
        "        for i in range(N):\n",
        "            # beta [0 ~1]\n",
        "            beta = rand()\n",
        "            # frequency (2)\n",
        "            freq = fmin + (fmax - fmin) * beta\n",
        "            for d in range(dim):\n",
        "                # Velocity update (3)\n",
        "                V[i,d]    = V[i,d] + (X[i,d] - Xgb[0,d]) * freq \n",
        "                # Position update (4)\n",
        "                Xnew[i,d] = X[i,d] + V[i,d]\n",
        "                # Boundary\n",
        "                Xnew[i,d] = boundary(Xnew[i,d], lb[0,d], ub[0,d])\n",
        "                \n",
        "            # Generate local solution around best solution\n",
        "            if rand() > r[i]:\n",
        "                for d in range (dim):\n",
        "                    # Epsilon in [-1,1]\n",
        "                    eps       = -1 + 2 * rand()\n",
        "                    # Random walk (5)\n",
        "                    Xnew[i,d] = Xgb[0,d] + eps * np.mean(A)              \n",
        "                    # Boundary\n",
        "                    Xnew[i,d] = boundary(Xnew[i,d], lb[0,d], ub[0,d])\n",
        "            \n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(Xnew, thres, N, dim)\n",
        "        \n",
        "        # Greedy selection\n",
        "        for i in range(N):\n",
        "            Fnew = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if (rand() < A[i])  and  (Fnew <= fit[i,0]):\n",
        "                X[i,:]   = Xnew[i,:]\n",
        "                fit[i,0] = Fnew\n",
        "                # Loudness update (6)\n",
        "                A[i]     = alpha * A[i]\n",
        "                # Pulse rate update (6)\n",
        "                r[i]     = r0[i] * (1 - np.exp(-gamma * t))               \n",
        "                \n",
        "            if fit[i,0] < fitG:\n",
        "                Xgb[0,:] = X[i,:]\n",
        "                fitG     = fit[i,0]\n",
        "             \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Generation:\", t + 1)\n",
        "        print(\"Best (BA):\", curve[0,t])\n",
        "        t += 1            \n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "            \n",
        "    # # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # ba_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return ba_data  \n",
        "    return\n",
        "\n",
        "\n",
        "            \n",
        "            \n",
        "                "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOWoAsoYtyxm"
      },
      "source": [
        "## FPA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNl1zWpevCaj"
      },
      "source": [
        "#[2012]-\"Flower pollination algorithm for global optimization\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "import math\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "# Levy Flight\n",
        "def levy_distribution(beta, dim):\n",
        "    # Sigma     \n",
        "    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n",
        "    deno  = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)\n",
        "    sigma = (nume / deno) ** (1 / beta) \n",
        "    # Parameter u & v \n",
        "    u     = np.random.randn(dim) * sigma\n",
        "    v     = np.random.randn(dim)\n",
        "    # Step \n",
        "    step  = u / abs(v) ** (1 / beta)\n",
        "    LF    = 0.01 * step\n",
        "    \n",
        "    return LF\n",
        "\n",
        "\n",
        "def FPA(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub     = 1\n",
        "    lb     = 0\n",
        "    thres  = 0.5\n",
        "    beta   = 1.5    # levy component\n",
        "    P      = 0.8    # switch probability\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    if 'P' in opts:\n",
        "        P    = opts['P'] \n",
        "    if 'beta' in opts:\n",
        "        beta = opts['beta'] \n",
        "        \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Binary conversion\n",
        "    Xbin  = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xgb   = np.zeros([1, dim], dtype='float')\n",
        "    fitG  = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "        if fit[i,0] < fitG:\n",
        "            Xgb[0,:] = X[i,:]\n",
        "            fitG     = fit[i,0]\n",
        "    \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = fitG.copy()\n",
        "    print(\"Generation:\", t + 1)\n",
        "    print(\"Best (FPA):\", curve[0,t])\n",
        "    t += 1\n",
        "    \n",
        "    while t < max_iter:  \n",
        "        ##added\n",
        "        c=t  \n",
        "        Xnew  = np.zeros([N, dim], dtype='float') \n",
        "        \n",
        "        for i in range(N):\n",
        "            # Global pollination \n",
        "            if rand() < P:\n",
        "                # Levy distribution (2)\n",
        "                L = levy_distribution(beta, dim) \n",
        "                for d in range(dim):\n",
        "                    # Global pollination (1)\n",
        "                    Xnew[i,d] = X[i,d] + L[d] * (X[i,d] - Xgb[0,d]) \n",
        "                    # Boundary\n",
        "                    Xnew[i,d] = boundary(Xnew[i,d], lb[0,d], ub[0,d])\n",
        "                    \n",
        "            # Local pollination\n",
        "            else:\n",
        "                # Different flower j, k in same species\n",
        "                R   = np.random.permutation(N) \n",
        "                J   = R[0] \n",
        "                K   = R[1]\n",
        "                # Epsilon [0 to 1]\n",
        "                eps = rand()\n",
        "                for d in range(dim):\n",
        "                    # Local pollination (3)\n",
        "                    Xnew[i,d] = X[i,d] + eps * (X[J,d] - X[K,d])           \n",
        "                    # Boundary\n",
        "                    Xnew[i,d] = boundary(Xnew[i,d], lb[0,d], ub[0,d])\n",
        "                \n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(Xnew, thres, N, dim)\n",
        "        \n",
        "        # Greedy selection\n",
        "        for i in range(N):\n",
        "            Fnew = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if Fnew <= fit[i,0]:\n",
        "                X[i,:]   = Xnew[i,:]\n",
        "                fit[i,0] = Fnew            \n",
        "                \n",
        "            if fit[i,0] < fitG:\n",
        "                Xgb[0,:] = X[i,:]\n",
        "                fitG     = fit[i,0]\n",
        "             \n",
        "        # Store result\n",
        "        curve[0,t] = fitG.copy()\n",
        "        print(\"Generation:\", t + 1)\n",
        "        print(\"Best (FPA):\", curve[0,t])\n",
        "        t += 1            \n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "            \n",
        "    # # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xgb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # fpa_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return fpa_data  \n",
        "    return\n",
        "\n",
        "            \n",
        "            \n",
        "                "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TQRMZtQwYq_"
      },
      "source": [
        "## GWO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtkreeRLwaLm"
      },
      "source": [
        "#[2014]-\"Grey wolf optimizer\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "def GWO(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub    = 1\n",
        "    lb    = 0\n",
        "    thres = 0.5\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X      = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Binary conversion\n",
        "    Xbin   = binary_conversion(X, thres, N, dim)\n",
        "    \n",
        "    # Fitness at first iteration\n",
        "    fit    = np.zeros([N, 1], dtype='float')\n",
        "    Xalpha = np.zeros([1, dim], dtype='float')\n",
        "    Xbeta  = np.zeros([1, dim], dtype='float')\n",
        "    Xdelta = np.zeros([1, dim], dtype='float')\n",
        "    Falpha = float('inf')\n",
        "    Fbeta  = float('inf')\n",
        "    Fdelta = float('inf')\n",
        "    \n",
        "    for i in range(N):\n",
        "        fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "        if fit[i,0] < Falpha:\n",
        "            Xalpha[0,:] = X[i,:]\n",
        "            Falpha      = fit[i,0]\n",
        "            \n",
        "        if fit[i,0] < Fbeta and fit[i,0] > Falpha:\n",
        "            Xbeta[0,:]  = X[i,:]\n",
        "            Fbeta       = fit[i,0]\n",
        "            \n",
        "        if fit[i,0] < Fdelta and fit[i,0] > Fbeta and fit[i,0] > Falpha:\n",
        "            Xdelta[0,:] = X[i,:]\n",
        "            Fdelta      = fit[i,0]\n",
        "    \n",
        "    # Pre\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    curve[0,t] = Falpha.copy()\n",
        "    print(\"Iteration:\", t + 1)\n",
        "    print(\"Best (GWO):\", curve[0,t])\n",
        "    t += 1\n",
        "    \n",
        "    while t < max_iter:  \n",
        "        ##added\n",
        "        c=t \n",
        "      \t# Coefficient decreases linearly from 2 to 0 \n",
        "        a = 2 - t * (2 / max_iter) \n",
        "        \n",
        "        for i in range(N):\n",
        "            for d in range(dim):\n",
        "                # Parameter C (3.4)\n",
        "                C1     = 2 * rand()\n",
        "                C2     = 2 * rand()\n",
        "                C3     = 2 * rand()\n",
        "                # Compute Dalpha, Dbeta & Ddelta (3.5)\n",
        "                Dalpha = abs(C1 * Xalpha[0,d] - X[i,d]) \n",
        "                Dbeta  = abs(C2 * Xbeta[0,d] - X[i,d])\n",
        "                Ddelta = abs(C3 * Xdelta[0,d] - X[i,d])\n",
        "                # Parameter A (3.3)\n",
        "                A1     = 2 * a * rand() - a\n",
        "                A2     = 2 * a * rand() - a\n",
        "                A3     = 2 * a * rand() - a\n",
        "                # Compute X1, X2 & X3 (3.6) \n",
        "                X1     = Xalpha[0,d] - A1 * Dalpha\n",
        "                X2     = Xbeta[0,d] - A2 * Dbeta\n",
        "                X3     = Xdelta[0,d] - A3 * Ddelta\n",
        "                # Update wolf (3.7)\n",
        "                X[i,d] = (X1 + X2 + X3) / 3                \n",
        "                # Boundary\n",
        "                X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "        \n",
        "        # Binary conversion\n",
        "        Xbin  = binary_conversion(X, thres, N, dim)\n",
        "        \n",
        "        # Fitness\n",
        "        for i in range(N):\n",
        "            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if fit[i,0] < Falpha:\n",
        "                Xalpha[0,:] = X[i,:]\n",
        "                Falpha      = fit[i,0]\n",
        "                \n",
        "            if fit[i,0] < Fbeta and fit[i,0] > Falpha:\n",
        "                Xbeta[0,:]  = X[i,:]\n",
        "                Fbeta       = fit[i,0]\n",
        "                \n",
        "            if fit[i,0] < Fdelta and fit[i,0] > Fbeta and fit[i,0] > Falpha:\n",
        "                Xdelta[0,:] = X[i,:]\n",
        "                Fdelta      = fit[i,0]\n",
        "        \n",
        "        curve[0,t] = Falpha.copy()\n",
        "        print(\"Iteration:\", t + 1)\n",
        "        print(\"Best (GWO):\", curve[0,t])\n",
        "        t += 1\n",
        "        if True:\n",
        "            Gbin       = binary_conversion(Xalpha, thres, 1, dim) \n",
        "            Gbin       = Gbin.reshape(dim)\n",
        "            pos        = np.asarray(range(0, dim))    \n",
        "            sel_index  = pos[Gbin == 1]\n",
        "            num_feat   = len(sel_index)\n",
        "            done=time.time()\n",
        "            elapsed=done-start\n",
        "            elapsed+=time_cal\n",
        "            time_cal=elapsed\n",
        "            print((\"Total time taken: %f\")%(time_cal/60))\n",
        "            rt=time_cal/60\n",
        "            save_file(ds_name,t-1,sel_index,N,rt)\n",
        "            start=time.time()\n",
        "                \n",
        "    # # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xalpha, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # gwo_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return gwo_data \n",
        "    return\n",
        "        \n",
        "                \n",
        "                \n",
        "                \n",
        "    \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5yP1fom10l2"
      },
      "source": [
        "## HHO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRhkyza115l"
      },
      "source": [
        "#[2019]-\"Harris hawks optimization: Algorithm and applications\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "import math\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def levy_distribution(beta, dim):\n",
        "    # Sigma \n",
        "    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n",
        "    deno  = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)\n",
        "    sigma = (nume / deno) ** (1 / beta)\n",
        "    # Parameter u & v \n",
        "    u     = np.random.randn(dim) * sigma\n",
        "    v     = np.random.randn(dim)\n",
        "    # Step \n",
        "    step  = u / abs(v) ** (1 / beta)\n",
        "    LF    = 0.01 * step    \n",
        "\n",
        "    return LF\n",
        "\n",
        "\n",
        "def HHO(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub    = 1\n",
        "    lb    = 0\n",
        "    thres = 0.5\n",
        "    beta  = 1.5    # levy component\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    if 'beta' in opts:\n",
        "        beta = opts['beta']\n",
        "        \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Pre\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xrb   = np.zeros([1, dim], dtype='float')\n",
        "    fitR  = float('inf')\n",
        "            \n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "    \n",
        "    while t < max_iter:\n",
        "        ##added\n",
        "        c=t \n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(X, thres, N, dim)\n",
        "        \n",
        "        # Fitness\n",
        "        for i in range(N):\n",
        "            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if fit[i,0] < fitR:\n",
        "                Xrb[0,:] = X[i,:]\n",
        "                fitR     = fit[i,0]\n",
        "                \n",
        "        # Store result\n",
        "        curve[0,t] = fitR.copy()\n",
        "        print(\"Iteration:\", t + 1)\n",
        "        print(\"Best (HHO):\", curve[0,t])\n",
        "        t += 1\n",
        "\n",
        "        # Mean position of hawk (2)\n",
        "        X_mu      = np.zeros([1, dim], dtype='float')\n",
        "        X_mu[0,:] = np.mean(X, axis=0)\n",
        "        \n",
        "        for i in range(N):\n",
        "            # Random number in [-1,1]\n",
        "            E0 = -1 + 2 * rand()\n",
        "            # Escaping energy of rabbit (3)\n",
        "            E  = 2 * E0 * (1 - (t / max_iter)) \n",
        "            # Exploration phase\n",
        "            if abs(E) >= 1:\n",
        "                # Define q in [0,1]\n",
        "                q = rand()\n",
        "                if q >= 0.5:\n",
        "                    # Random select a hawk k\n",
        "                    k  = np.random.randint(low = 0, high = N)\n",
        "                    r1 = rand()\n",
        "                    r2 = rand()\n",
        "                    for d in range(dim):\n",
        "                        # Position update (1)\n",
        "                        X[i,d] = X[k,d] - r1 * abs(X[k,d] - 2 * r2 * X[i,d])\n",
        "                        # Boundary\n",
        "                        X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "\n",
        "                elif q < 0.5:    \n",
        "                    r3 = rand() \n",
        "                    r4 = rand()\n",
        "                    for d in range(dim):\n",
        "                        # Update Hawk (1)\n",
        "                        X[i,d] = (Xrb[0,d] - X_mu[0,d]) - r3 * (lb[0,d] + r4 * (ub[0,d] - lb[0,d]))\n",
        "                        # Boundary\n",
        "                        X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "                        \n",
        "            # Exploitation phase \n",
        "            elif abs(E) < 1:\n",
        "                # Jump strength \n",
        "                J = 2 * (1 - rand()) \n",
        "                r = rand()\n",
        "                # {1} Soft besiege\n",
        "                if r >= 0.5 and abs(E) >= 0.5:\n",
        "                    for d in range(dim):\n",
        "                        # Delta X (5)\n",
        "                        DX     = Xrb[0,d] - X[i,d]\n",
        "                        # Position update (4)\n",
        "                        X[i,d] = DX - E * abs(J * Xrb[0,d] - X[i,d])\n",
        "                        # Boundary\n",
        "                        X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "                        \n",
        "                # {2} hard besiege\n",
        "                elif r >= 0.5 and abs(E) < 0.5:\n",
        "                    for d in range(dim):\n",
        "                        # Delta X (5)\n",
        "                        DX     = Xrb[0,d] - X[i,d]\n",
        "                        # Position update (6)\n",
        "                        X[i,d] = Xrb[0,d] - E * abs(DX)    \n",
        "                        # Boundary\n",
        "                        X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
        "                        \n",
        "                # {3} Soft besiege with progressive rapid dives\n",
        "                elif r < 0.5 and abs(E) >= 0.5:\n",
        "                    # Levy distribution (9)\n",
        "                    LF = levy_distribution(beta, dim) \n",
        "                    Y  = np.zeros([1, dim], dtype='float')\n",
        "                    Z  = np.zeros([1, dim], dtype='float')\n",
        "                    \n",
        "                    for d in range(dim):\n",
        "                        # Compute Y (7)\n",
        "                        Y[0,d] = Xrb[0,d] - E * abs(J * Xrb[0,d] - X[i,d])\n",
        "                        # Boundary\n",
        "                        Y[0,d] = boundary(Y[0,d], lb[0,d], ub[0,d])\n",
        "                        \n",
        "                    for d in range(dim):\n",
        "                        # Compute Z (8)\n",
        "                        Z[0,d] = Y[0,d] + rand() * LF[d]\n",
        "                        # Boundary\n",
        "                        Z[0,d] = boundary(Z[0,d], lb[0,d], ub[0,d])          \n",
        "                    \n",
        "                    # Binary conversion\n",
        "                    Ybin = binary_conversion(Y, thres, 1, dim)\n",
        "                    Zbin = binary_conversion(Z, thres, 1, dim)\n",
        "                    # fitness\n",
        "                    fitY = Fun(xtrain, ytrain, Ybin[0,:], opts)\n",
        "                    fitZ = Fun(xtrain, ytrain, Zbin[0,:], opts)\n",
        "                    # Greedy selection (10)\n",
        "                    if fitY < fit[i,0]:\n",
        "                        fit[i,0]  = fitY \n",
        "                        X[i,:]    = Y[0,:]\n",
        "                    if fitZ < fit[i,0]:\n",
        "                        fit[i,0]  = fitZ\n",
        "                        X[i,:]    = Z[0,:]                        \n",
        "\n",
        "                # {4} Hard besiege with progressive rapid dives\n",
        "                elif r < 0.5 and abs(E) < 0.5:\n",
        "                    # Levy distribution (9)\n",
        "                    LF = levy_distribution(beta, dim) \n",
        "                    Y  = np.zeros([1, dim], dtype='float')\n",
        "                    Z  = np.zeros([1, dim], dtype='float')\n",
        "                    \n",
        "                    for d in range(dim):\n",
        "                        # Compute Y (12)\n",
        "                        Y[0,d] = Xrb[0,d] - E * abs(J * Xrb[0,d] - X_mu[0,d])\n",
        "                        # Boundary\n",
        "                        Y[0,d] = boundary(Y[0,d], lb[0,d], ub[0,d])\n",
        "                    \n",
        "                    for d in range(dim):\n",
        "                        # Compute Z (13)\n",
        "                        Z[0,d] = Y[0,d] + rand() * LF[d]\n",
        "                        # Boundary\n",
        "                        Z[0,d] = boundary(Z[0,d], lb[0,d], ub[0,d])    \n",
        "\n",
        "                    # Binary conversion\n",
        "                    Ybin = binary_conversion(Y, thres, 1, dim)\n",
        "                    Zbin = binary_conversion(Z, thres, 1, dim)\n",
        "                    # fitness\n",
        "                    fitY = Fun(xtrain, ytrain, Ybin[0,:], opts)\n",
        "                    fitZ = Fun(xtrain, ytrain, Zbin[0,:], opts)\n",
        "                    # Greedy selection (10)\n",
        "                    if fitY < fit[i,0]:\n",
        "                        fit[i,0]  = fitY\n",
        "                        X[i,:]    = Y[0,:]\n",
        "                    if fitZ < fit[i,0]:\n",
        "                        fit[i,0]  = fitZ\n",
        "                        X[i,:]    = Z[0,:]  \n",
        "        if True:\n",
        "              Gbin       = binary_conversion(Xrb, thres, 1, dim) \n",
        "              Gbin       = Gbin.reshape(dim)\n",
        "              pos        = np.asarray(range(0, dim))    \n",
        "              sel_index  = pos[Gbin == 1]\n",
        "              num_feat   = len(sel_index)\n",
        "              done=time.time()\n",
        "              elapsed=done-start\n",
        "              elapsed+=time_cal\n",
        "              time_cal=elapsed\n",
        "              print((\"Total time taken: %f\")%(time_cal/60))\n",
        "              rt=time_cal/60\n",
        "              save_file(ds_name,t,sel_index,N,rt)\n",
        "              start=time.time()\n",
        "\n",
        "    # # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xrb, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # hho_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return hho_data    \n",
        "    return                       \n",
        "                        \n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIfGvOMj3Q2r"
      },
      "source": [
        "## SSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nGbF0kr3SSL"
      },
      "source": [
        "#[2017]-\"Salp swarm algorithm: A bio-inspired optimizer for engineering design problems\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import rand\n",
        "#from FS.functionHO import Fun\n",
        "\n",
        "\n",
        "def init_position(lb, ub, N, dim):\n",
        "    X = np.zeros([N, dim], dtype='float')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def binary_conversion(X, thres, N, dim):\n",
        "    Xbin = np.zeros([N, dim], dtype='int')\n",
        "    for i in range(N):\n",
        "        for d in range(dim):\n",
        "            if X[i,d] > thres:\n",
        "                Xbin[i,d] = 1\n",
        "            else:\n",
        "                Xbin[i,d] = 0\n",
        "    \n",
        "    return Xbin\n",
        "\n",
        "\n",
        "def boundary(x, lb, ub):\n",
        "    if x < lb:\n",
        "        x = lb\n",
        "    if x > ub:\n",
        "        x = ub\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def SSA(xtrain, ytrain, opts):\n",
        "    ##added\n",
        "    start = time.time()\n",
        "    time_cal=0\n",
        "    # Parameters\n",
        "    ub    = 1\n",
        "    lb    = 0\n",
        "    thres = 0.5\n",
        "    \n",
        "    N        = opts['N']\n",
        "    max_iter = opts['T']\n",
        "    \n",
        "    # Dimension\n",
        "    dim = np.size(xtrain, 1)\n",
        "    if np.size(lb) == 1:\n",
        "        ub = ub * np.ones([1, dim], dtype='float')\n",
        "        lb = lb * np.ones([1, dim], dtype='float')\n",
        "        \n",
        "    # Initialize position \n",
        "    X     = init_position(lb, ub, N, dim)\n",
        "    \n",
        "    # Pre\n",
        "    fit   = np.zeros([N, 1], dtype='float')\n",
        "    Xf    = np.zeros([1, dim], dtype='float')\n",
        "    fitF  = float('inf')\n",
        "    curve = np.zeros([1, max_iter], dtype='float') \n",
        "    t     = 0\n",
        "\n",
        "    while t < max_iter:\n",
        "        ##added\n",
        "        c=t \n",
        "        # Binary conversion\n",
        "        Xbin = binary_conversion(X, thres, N, dim)\n",
        "        \n",
        "        # Fitness\n",
        "        for i in range(N):\n",
        "            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
        "            if fit[i,0] < fitF:\n",
        "                Xf[0,:] = X[i,:]\n",
        "                fitF    = fit[i,0]\n",
        "        \n",
        "        # Store result\n",
        "        curve[0,t] = fitF.copy()\n",
        "        print(\"Iteration:\", t + 1)\n",
        "        print(\"Best (SSA):\", curve[0,t])\n",
        "        t += 1\n",
        "        \n",
        " \t    # Compute coefficient, c1 (3.2)\n",
        "        c1 = 2 * np.exp(-(4 * t / max_iter) ** 2)\n",
        "        \n",
        "        for i in range(N):          \n",
        "            # First leader update\n",
        "            if i == 0:  \n",
        "                for d in range(dim):\n",
        "                    # Coefficient c2 & c3 [0 ~ 1]\n",
        "                    c2 = rand() \n",
        "                    c3 = rand()\n",
        "              \t    # Leader update (3.1)\n",
        "                    if c3 >= 0.5: \n",
        "                        X[i,d] = Xf[0,d] + c1 * ((ub[0,d] - lb[0,d]) * c2 + lb[0,d])\n",
        "                    else:\n",
        "                        X[i,d] = Xf[0,d] - c1 * ((ub[0,d] - lb[0,d]) * c2 + lb[0,d])\n",
        "                \n",
        "                    # Boundary\n",
        "                    X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d]) \n",
        "                \n",
        "            # Salp update\n",
        "            elif i >= 1:\n",
        "                for d in range(dim):\n",
        "                    # Salp update by following front salp (3.4)\n",
        "                    X[i,d] = (X[i,d] + X[i-1, d]) / 2\n",
        "                    # Boundary\n",
        "                    X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d]) \n",
        "        \n",
        "        if True:\n",
        "              Gbin       = binary_conversion(Xf, thres, 1, dim) \n",
        "              Gbin       = Gbin.reshape(dim)\n",
        "              pos        = np.asarray(range(0, dim))    \n",
        "              sel_index  = pos[Gbin == 1]\n",
        "              num_feat   = len(sel_index)\n",
        "              done=time.time()\n",
        "              elapsed=done-start\n",
        "              elapsed+=time_cal\n",
        "              time_cal=elapsed\n",
        "              print((\"Total time taken: %f\")%(time_cal/60))\n",
        "              rt=time_cal/60\n",
        "              save_file(ds_name,t,sel_index,N,rt)\n",
        "              start=time.time()\n",
        "    # # Best feature subset\n",
        "    # Gbin       = binary_conversion(Xf, thres, 1, dim) \n",
        "    # Gbin       = Gbin.reshape(dim)\n",
        "    # pos        = np.asarray(range(0, dim))    \n",
        "    # sel_index  = pos[Gbin == 1]\n",
        "    # num_feat   = len(sel_index)\n",
        "    # # Create dictionary\n",
        "    # ssa_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
        "    \n",
        "    # return ssa_data \n",
        "    return "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLvUKGNzrAv_"
      },
      "source": [
        "## k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz4ko7hZEKdA"
      },
      "source": [
        "def plot_pdf(clf):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from matplotlib.backends.backend_pdf import PdfPages\n",
        "  import pandas as pd\n",
        "  from matplotlib.pyplot import figure\n",
        "  figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "  df=pd.read_csv(('%s/%s.csv')%(results_csv,clf))\n",
        "  colunms=df.columns\n",
        "  i=0\n",
        "  with PdfPages(('%s/%s.pdf')%(results_graphs,clf)) as pdf:\n",
        "    while True:\n",
        "      if i>=df.shape[1]:\n",
        "        break\n",
        "      df[df.columns[i:i+1]].plot(subplots=True)\n",
        "      pdf.savefig()\n",
        "      i=i+1\n",
        "\n",
        "\n",
        "def mean_clf(clf):\n",
        "  df=pd.read_csv(('%s/%s.csv')%(results_csv,clf))\n",
        "  temp=((df.mean(axis = 0,skipna = True))) \n",
        "  print(temp)\n",
        "  #(pd.DataFrame(df.mean(axis = 0,skipna = True)).T).to_csv((\"%stest.csv\")%(clf))\n",
        "  temp.to_csv((\"%s/%s_average.csv\")%(results_csv,clf))\n",
        "  \n",
        "  \n",
        "  \n",
        "def add_result_to_csv(score,fn,nsf,t,npp,rt):\n",
        "  import csv\n",
        "  fieldnames = ['Iteration','# of particles','Total # of features','# of features selected','Running Time of FS','Fit-time','Score-time','Train_Accuracy','Test_Accuracy','Train_precision','Test_precision','Train_recall','Test_recall','Train_F-measure','Test_F-measure','Train_AUC','Test_AUC']\n",
        "  file_exists = os.path.isfile(('%s.csv')%(fn))\n",
        "  with open(('%s.csv')%(fn), 'a', newline='') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      if not file_exists:\n",
        "        writer.writeheader() \n",
        "      for i in score.keys():\n",
        "        score[i]=np.mean(score[i])\n",
        "      try:\n",
        "        writer.writerow({'Iteration':t,'# of particles':npp,'Total # of features':n_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro'],'Train_AUC':score['train_roc_auc_ovr'],'Test_AUC':score['test_roc_auc_ovr']})\n",
        "      except:\n",
        "        writer.writerow({'Iteration':t,'# of particles':npp,'Total # of features':n_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro']})\n",
        "  return\n",
        "  \n",
        "\n",
        "\n",
        "def nb_tree_svm(df,t,npp,rt,gbest_tracer):\n",
        "  X=df.values\n",
        "  y=X[:,-1]\n",
        "  X=X[:,:-1]\n",
        "  from sklearn.model_selection import KFold, cross_val_score\n",
        "  from sklearn import svm\n",
        "  from sklearn import tree\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  from sklearn.model_selection import StratifiedKFold\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.model_selection import KFold\n",
        "  #from sklearn.calibration import CalibratedClassifiertCV\n",
        "  #from sklearn.multiclass import OneVsRestClassifier\n",
        "  \n",
        "  try:\n",
        "    #print(e)\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    clf = GaussianNB()\n",
        "    scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "    #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "    nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  except :\n",
        "    try:\n",
        "      k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      clf = GaussianNB()\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      clf = GaussianNB()\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      nb_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  fn=('%s/naive bayes')%(results_csv)\n",
        "  add_result_to_csv(nb_score,fn,df.shape[1]-1,t,npp,rt,gbest_tracer)\n",
        "  #########################################################################################################\n",
        "  try:\n",
        "    #print(e)\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "    #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "    clf = tree.DecisionTreeClassifier()\n",
        "    tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  except:\n",
        "    try:\n",
        "      k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      clf = tree.DecisionTreeClassifier()\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      #print(e)\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      clf = tree.DecisionTreeClassifier()\n",
        "      tree_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  fn=('%s/tree')%(results_csv)\n",
        "  add_result_to_csv(tree_score,fn,df.shape[1]-1,t,npp,rt,gbest_tracer)\n",
        "  #######################################################################################################\n",
        "  if True:\n",
        "    try:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      clf = svm.SVC(kernel='linear',probability=True)\n",
        "      svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      try:\n",
        "        k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "      except:\n",
        "        k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    fn=('%s/svm_linear')%(results_csv)\n",
        "    add_result_to_csv(svm_score,fn,df.shape[1]-1,t,npp,rt,gbest_tracer)\n",
        "  else:\n",
        "    try:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      #clf = svm.SVC(kernel='linear',probability=True)\n",
        "      clf=svm.LinearSVC()\n",
        "      svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      try:\n",
        "        k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "      except:\n",
        "        k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "        scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "        clf = svm.SVC(kernel='linear',probability=True)\n",
        "        svm_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    fn=('%s/svm_linear')%(results_csv)\n",
        "    add_result_to_csv(svm_score,fn,df.shape[1]-1,t,npp,rt,gbest_tracer)\n",
        "  ########################################################################################################\n",
        "  try:\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "    #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "    clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "    knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  except:\n",
        "    try:\n",
        "      k_fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "      knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "    except:\n",
        "      k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "      #scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro','roc_auc_ovr']\n",
        "      scoring = ['accuracy', 'precision_macro','recall_macro','f1_micro']\n",
        "      clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "      knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1, return_train_score=True)\n",
        "  fn=('%s/knn')%(results_csv)\n",
        "  add_result_to_csv(knn_score,fn,df.shape[1]-1,t,npp,rt,gbest_tracer)\n",
        "  return nb_score,tree_score,svm_score,knn_score\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "def save_file(ds_name,t,sel_index,npp,rt):\n",
        "  print(\"Save reduced dataset at iteration: \"+str(t))\n",
        "  try:\n",
        "    print(\"NO. of Features Selected:\",len(sel_index))\n",
        "    print(\"Selected Features: \",sel_index)\n",
        "  except:\n",
        "    pass\n",
        "  target_no=n_features-1\n",
        "  g_elite_comb=sel_index\n",
        "  g_elite_comb=g_elite_comb.astype(int)\n",
        "  X_fs=data[:,g_elite_comb]\n",
        "  g_elite_comb=np.append(g_elite_comb,target_no)\n",
        "  #g_elite_comb=np.array(g_elite_comb)\n",
        "  g_elite_comb=g_elite_comb.astype(int)\n",
        "  X_gfs=data[:,g_elite_comb]\n",
        "  y=data[:,target_no]\n",
        "  y_c_dim=y.reshape(-1, 1) \n",
        "  #print(X_gfs)\n",
        "  DF = pd.DataFrame(X_gfs)\n",
        "  DF.columns=df.columns[g_elite_comb]\n",
        "  #DF['class']=y\n",
        "  \n",
        "  #print time\n",
        "  # now = datetime.now()\n",
        "  # current_time = now.strftime(\"%H:%M:%S\")\n",
        "  # print(\"Current Time =\", current_time)\n",
        "  #done = time.time()\n",
        "  #elapsed = done - start\n",
        "  DF.to_csv(('%s/iteration%d.csv')%(reduced_datasets,t),index=False) \n",
        "  gbest_tracer=knn(X_fs,y_c_dim)\n",
        "  # nbs,tres,svms,knns=nb_tree_svm(DF,t,npp,rt,gbest_tracer)\n",
        "  # print(\"Naive Bayes Train-accuracy: \",nbs['train_accuracy'],\" Test-accuracy: \",nbs['test_accuracy'])\n",
        "  # print(\"Trees Train-accuracy: \",tres['train_accuracy'],\" Test-accuracy: \",tres['test_accuracy'])\n",
        "  # print(\"SVM Linear Train-accuracy: \",svms['train_accuracy'],\" Test-accuracy: \",svms['test_accuracy'])\n",
        "  # print(\"Knn Train-accuracy: \",knns['train_accuracy'],\" Test-accuracy: \",knns['test_accuracy'])\n",
        "  # import csv   \n",
        "  # fields=[ds_name,c,n_features,len(g_elite_comb),elapsed/60,len(SN),np.sum(SN),nbs,tres,svms,knns]\n",
        "  # with open(r'/content/drive/My Drive/FYP/Result/VS_CCPSO_Result.csv', 'a') as f:\n",
        "  #   writer = csv.writer(f)\n",
        "  #   writer.writerow(fields)\n",
        "  #print(\"Time Elapsed in (min):\",elapsed/60)\n",
        "  \n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8yAy5UBEQHu"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGePXzQEhDKB"
      },
      "source": [
        "# def nb_tree_svm(df):\n",
        "#   X=df.values\n",
        "#   y=X[:,-1]\n",
        "#   X=X[:,:-1]\n",
        "#   from sklearn.model_selection import KFold, cross_val_score\n",
        "#   from sklearn import svm\n",
        "#   from sklearn import tree\n",
        "#   from sklearn.naive_bayes import GaussianNB\n",
        "#   from sklearn.neighbors import KNeighborsClassifier\n",
        "#   from sklearn.model_selection import cross_val_score\n",
        "#   k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "#   clf = GaussianNB()\n",
        "#   print(\"Score for naive bayes:\")\n",
        "#   #print (np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1)))\n",
        "#   nb_score=(np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1)))\n",
        "#   print(nb_score)\n",
        "#   clf = tree.DecisionTreeClassifier()\n",
        "#   print(\"Score for tree:\")\n",
        "#   #print (np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1)))\n",
        "#   tree_score=(np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1)))\n",
        "#   print(tree_score)\n",
        "#   clf = svm.LinearSVC()\n",
        "#   print(\"Score for svm linear:\")\n",
        "#   #print (np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1)))\n",
        "#   svm_score=(np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1)))\n",
        "#   print(svm_score)\n",
        "#   print(\"Score for Knn\")\n",
        "#   # clf=KNeighborsClassifier(n_neighbors=7,p=1,n_jobs=-1)\n",
        "#   from sklearn.neighbors import KNeighborsClassifier\n",
        "#   from sklearn.model_selection import cross_val_score\n",
        "#   from sklearn.model_selection import LeaveOneOut\n",
        "#   #import numpy as np\n",
        "#   #create a new KNN model\n",
        "#   #knn_cv = KNeighborsClassifier(n_neighbors=5) \n",
        "#   #train model with cv of 5 \n",
        "#   k_fold = LeaveOneOut()\n",
        "#   clf = KNeighborsClassifier(n_neighbors=1,p=1)\n",
        "#   knn_score=(np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1)))\n",
        "#   print(knn_score)\n",
        "#   ###extra\n",
        "#   # knn_cv = KNeighborsClassifier(n_neighbors=7,p=1,n_jobs=-1)\n",
        "#   # #train model with cv of 5 \n",
        "#   # cv_scores = cross_val_score(knn_cv, X, y, cv=10)\n",
        "#   # #print each cv score (accuracy) and average them\n",
        "#   # #print(cv_scores)\n",
        "#   # #print(cv_scores mean:{}.format(np.mean(cv_scores)))\n",
        "#   # knn_score=np.mean(cv_scores)\n",
        "#   # print(knn_score)\n",
        "#   #return nb_score,tree_score,svm_score,knn_score"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQAGpUDTCbJz"
      },
      "source": [
        "def add_result_to_csv(score,fn,nsf,t,npp,rt,gbest_tracer):\n",
        "  import csv\n",
        "  fieldnames = ['Iteration','M','# of particles','Total # of features','# of features selected','Running Time of FS','Fit-time','Score-time','Train_Accuracy','Test_Accuracy','Train_precision','Test_precision','Train_recall','Test_recall','Train_F-measure','Test_F-measure','Train_AUC','Test_AUC','gbest']\n",
        "  file_exists = os.path.isfile(('%s.csv')%(fn))\n",
        "  with open(('%s.csv')%(fn), 'a', newline='') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      if not file_exists:\n",
        "        writer.writeheader() \n",
        "      for i in score.keys():\n",
        "        score[i]=np.mean(score[i])\n",
        "      try:\n",
        "        writer.writerow({'Iteration':t,'# of particles':npp,'Total # of features':n_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro'],'Train_AUC':score['train_roc_auc_ovr'],'Test_AUC':score['test_roc_auc_ovr'],'gbest':gbest_tracer})\n",
        "      except:\n",
        "        writer.writerow({'Iteration':t,'# of particles':npp,'Total # of features':n_features,'# of features selected':nsf,'Running Time of FS':rt,'Fit-time':score['fit_time'],'Score-time':score['score_time'],'Train_Accuracy':score['train_accuracy'],'Test_Accuracy':score['test_accuracy'],'Train_precision':score['train_precision_macro'],'Test_precision':score['test_precision_macro'],'Train_recall':score['train_recall_macro'],'Test_recall':score['test_recall_macro'],'Train_F-measure':score['train_f1_micro'],'Test_F-measure':score['test_f1_micro'],'gbest':gbest_tracer})\n",
        "  print(\"Gbest: \",gbest_tracer)\n",
        "  return"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tbJRPxEDAoz"
      },
      "source": [
        "def knn(X,y):\n",
        "  \"\"\"knn with Leave one out  cv\"\"\"\n",
        "  if X.shape[1]<1:\n",
        "    return 0\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  from sklearn.model_selection import LeaveOneOut\n",
        "  #import numpy as np\n",
        "  k_fold = LeaveOneOut()\n",
        "  clf = KNeighborsClassifier(n_neighbors=1,p=2)\n",
        "  #return np.mean(cross_val_score(clf, X, y, cv=k_fold, n_jobs=-1))\n",
        "  #scoring = ['balanced_accuracy']\n",
        "  scoring = ['accuracy']\n",
        "  knn_score=cross_validate(clf, X, y, cv=k_fold,scoring=scoring,n_jobs=-1)\n",
        "  #return np.mean(knn_score['test_balanced_accuracy'])\n",
        "  return np.mean(knn_score['test_accuracy'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLNOA37kUS86"
      },
      "source": [
        "# Main Modified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK5nXh25Ucw7"
      },
      "source": [
        "#common"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7M5uvPIUb3Z"
      },
      "source": [
        "from contextlib import redirect_stdout\r\n",
        "import datetime\r\n",
        "ds_name='Lymphoma'\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "enc = LabelEncoder()\r\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\r\n",
        "# load data\r\n",
        "#ds_name='dbworld_bodies'\r\n",
        "data  = pd.read_csv(('/content/drive/My Drive/FYP/Dataset/%s.csv')%(ds_name))\r\n",
        "data=data.replace('?',np.nan)\r\n",
        "enc.fit(data[data.columns[-1]])\r\n",
        "data[data.columns[-1]] = enc.transform(data[data.columns[-1]])\r\n",
        "imp=imp.fit(data)\r\n",
        "data_arr=imp.transform(data)\r\n",
        "data= pd.DataFrame(data=data_arr,columns=data.columns)\r\n",
        "# preprosed above\r\n",
        "df=data\r\n",
        "data  = data.values\r\n",
        "feat  = np.asarray(data[:, 0:-1])   # feature vector\r\n",
        "label = np.asarray(data[:, -1])     # label vector\r\n",
        "n_samples,n_features=data.shape\r\n",
        "# split data into train & validation (70 -- 30)\r\n",
        "#xtrain, xtest, ytrain, ytest = train_test_split(feat, label, test_size=0,shuffle=False)\r\n",
        "#fold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}\r\n",
        "fold=df\r\n",
        "\r\n",
        "gbest_tracer=0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qovAoO6iUqhO"
      },
      "source": [
        "# pso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8v351weUwgM"
      },
      "source": [
        "#######################################################\r\n",
        "## creating directories\r\n",
        "\r\n",
        "if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\r\n",
        "  os.makedirs(('/content/drive/My Drive/FYP/Existing'))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## Particle Swarm Optimization ( PSO )\r\n",
        "c=None\r\n",
        "rt=None\r\n",
        "algo_name='PSO'\r\n",
        "if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\r\n",
        "  os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\r\n",
        "\r\n",
        "hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\r\n",
        "if not os.path.exists(('%s/%s')%(hp,ds_name)):\r\n",
        "    reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "    results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "    temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "    results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "    os.makedirs(('%s/%s')%(hp,ds_name))\r\n",
        "    os.makedirs(reduced_datasets)\r\n",
        "    os.makedirs(results_csv)\r\n",
        "    os.makedirs(results_graphs)\r\n",
        "    os.makedirs(temp_storage)\r\n",
        "  \r\n",
        "else:\r\n",
        "  print(\"Error Already File Exists!!!!\")\r\n",
        "  reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "  results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "  temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "  results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "  import sys\r\n",
        "  #sys.exit(\"Error message\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# from FS.pso import jfs \r\n",
        "# from FS.utility import *\r\n",
        "# parameter\r\n",
        "k    = 5     # k-value in KNN\r\n",
        "N    = 100    # number of particles\r\n",
        "T    = 30   # maximum number of iterations\r\n",
        "w    = 0.9\r\n",
        "c1   = 2\r\n",
        "c2   = 2\r\n",
        "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'w':w, 'c1':c1, 'c2':c2}\r\n",
        "\r\n",
        "# perform feature selection\r\n",
        "PSO(feat, label, opts)\r\n",
        "# plot_pdf('naive bayes')\r\n",
        "# plot_pdf('tree')\r\n",
        "# plot_pdf('svm_linear')\r\n",
        "# plot_pdf('knn')\r\n",
        "\r\n",
        "# print(\"For Naive Bayes\")\r\n",
        "# mean_clf('naive bayes')\r\n",
        "# print(\"\\nFor Tree\")\r\n",
        "# mean_clf('tree')\r\n",
        "# print(\"\\nFor SVM Linear\")\r\n",
        "# mean_clf('svm_linear')\r\n",
        "# print(\"\\nFor KNN\")\r\n",
        "# mean_clf('knn')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbWxjpfpVRFx"
      },
      "source": [
        "# GA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjHgdwRfVSkB"
      },
      "source": [
        "###############################################################\r\n",
        "\r\n",
        "## creating directories\r\n",
        "\r\n",
        "if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\r\n",
        "  os.makedirs(('/content/drive/My Drive/FYP/Existing'))\r\n",
        "c=None\r\n",
        "rt=None\r\n",
        "algo_name='GA'\r\n",
        "if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\r\n",
        "  os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\r\n",
        "\r\n",
        "hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\r\n",
        "if not os.path.exists(('%s/%s')%(hp,ds_name)):\r\n",
        "    reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "    results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "    temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "    results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "    os.makedirs(('%s/%s')%(hp,ds_name))\r\n",
        "    os.makedirs(reduced_datasets)\r\n",
        "    os.makedirs(results_csv)\r\n",
        "    os.makedirs(results_graphs)\r\n",
        "    os.makedirs(temp_storage)\r\n",
        "  \r\n",
        "else:\r\n",
        "  print(\"Error Already File Exists!!!!\")\r\n",
        "  reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "  results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "  temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "  results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "  import sys\r\n",
        "  #sys.exit(\"Error message\")\r\n",
        "## Genetic Algorithm ( GA )\r\n",
        "\r\n",
        "#from FS.ga import jfs \r\n",
        "# parameter\r\n",
        "k    = 5     # k-value in KNN\r\n",
        "N    = 100    # number of chromosomes\r\n",
        "T    = 31   # maximum number of generations\r\n",
        "CR   = 0.8\r\n",
        "MR   = 0.01\r\n",
        "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'CR':CR, 'MR':MR}\r\n",
        "\r\n",
        "# perform feature selection\r\n",
        "GA(feat, label, opts)\r\n",
        "# plot_pdf('naive bayes')\r\n",
        "# plot_pdf('tree')\r\n",
        "# plot_pdf('svm_linear')\r\n",
        "# plot_pdf('knn')\r\n",
        "\r\n",
        "# print(\"For Naive Bayes\")\r\n",
        "# mean_clf('naive bayes')\r\n",
        "# print(\"\\nFor Tree\")\r\n",
        "# mean_clf('tree')\r\n",
        "# print(\"\\nFor SVM Linear\")\r\n",
        "# mean_clf('svm_linear')\r\n",
        "# print(\"\\nFor KNN\")\r\n",
        "# mean_clf('knn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqZrrk-rVciy"
      },
      "source": [
        "#  CS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfLUrpK-Vd4F"
      },
      "source": [
        "\r\n",
        "## creating directories\r\n",
        "\r\n",
        "if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\r\n",
        "  os.makedirs(('/content/drive/My Drive/FYP/Existing'))\r\n",
        "c=None\r\n",
        "rt=None\r\n",
        "algo_name='CS'\r\n",
        "if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\r\n",
        "  os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\r\n",
        "\r\n",
        "hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\r\n",
        "if not os.path.exists(('%s/%s')%(hp,ds_name)):\r\n",
        "    reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "    results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "    temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "    results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "    os.makedirs(('%s/%s')%(hp,ds_name))\r\n",
        "    os.makedirs(reduced_datasets)\r\n",
        "    os.makedirs(results_csv)\r\n",
        "    os.makedirs(results_graphs)\r\n",
        "    os.makedirs(temp_storage)\r\n",
        "  \r\n",
        "else:\r\n",
        "  print(\"Error Already File Exists!!!!\")\r\n",
        "  reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "  results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "  temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "  results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "  import sys\r\n",
        "  #sys.exit(\"Error message\")\r\n",
        "## Cuckoo Search (CS)\r\n",
        "#rom FS.cs import jfs \r\n",
        "k    = 5     # k-value in KNN\r\n",
        "N    = 100    # number of chromosomes\r\n",
        "T    = 31   # maximum number of generations\r\n",
        "Pa  = 0.25   # discovery rate\r\n",
        "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'Pa':Pa}\r\n",
        "\r\n",
        "# perform feature selection\r\n",
        "CS(feat, label, opts)\r\n",
        "# plot_pdf('naive bayes')\r\n",
        "# plot_pdf('tree')\r\n",
        "# plot_pdf('svm_linear')\r\n",
        "# plot_pdf('knn')\r\n",
        "\r\n",
        "# print(\"For Naive Bayes\")\r\n",
        "# mean_clf('naive bayes')\r\n",
        "# print(\"\\nFor Tree\")\r\n",
        "# mean_clf('tree')\r\n",
        "# print(\"\\nFor SVM Linear\")\r\n",
        "# mean_clf('svm_linear')\r\n",
        "# print(\"\\nFor KNN\")\r\n",
        "# mean_clf('knn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78wnWX5WVnw5"
      },
      "source": [
        "# DE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_wOlLufVvZR"
      },
      "source": [
        "\r\n",
        "## creating directories\r\n",
        "\r\n",
        "if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\r\n",
        "  os.makedirs(('/content/drive/My Drive/FYP/Existing'))\r\n",
        "c=None\r\n",
        "rt=None\r\n",
        "algo_name='DE'\r\n",
        "if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\r\n",
        "  os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\r\n",
        "\r\n",
        "hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\r\n",
        "if not os.path.exists(('%s/%s')%(hp,ds_name)):\r\n",
        "    reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "    results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "    temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "    results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "    os.makedirs(('%s/%s')%(hp,ds_name))\r\n",
        "    os.makedirs(reduced_datasets)\r\n",
        "    os.makedirs(results_csv)\r\n",
        "    os.makedirs(results_graphs)\r\n",
        "    os.makedirs(temp_storage)\r\n",
        "  \r\n",
        "else:\r\n",
        "  print(\"Error Already File Exists!!!!\")\r\n",
        "  reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "  results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "  temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "  results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "  import sys\r\n",
        "  #sys.exit(\"Error message\")\r\n",
        "##  Differential Evolution (DE)\r\n",
        "#from FS.de import jfs \r\n",
        "k    = 5     # k-value in KNN\r\n",
        "N    = 100    # number of chromosomes\r\n",
        "T    = 31   # maximum number of generations\r\n",
        "CR = 0.9    # crossover rate\r\n",
        "F  = 0.5    # constant factor\r\n",
        "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'CR':CR, 'F':F}\r\n",
        "# perform feature selection\r\n",
        "DE(feat, label, opts)\r\n",
        "# plot_pdf('naive bayes')\r\n",
        "# plot_pdf('tree')\r\n",
        "# plot_pdf('svm_linear')\r\n",
        "# plot_pdf('knn')\r\n",
        "\r\n",
        "# print(\"For Naive Bayes\")\r\n",
        "# mean_clf('naive bayes')\r\n",
        "# print(\"\\nFor Tree\")\r\n",
        "# mean_clf('tree')\r\n",
        "# print(\"\\nFor SVM Linear\")\r\n",
        "# mean_clf('svm_linear')\r\n",
        "# print(\"\\nFor KNN\")\r\n",
        "# mean_clf('knn')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdAjYbhmVynv"
      },
      "source": [
        "# WOA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk2VZSpfV7PC"
      },
      "source": [
        "\r\n",
        "## creating directories\r\n",
        "\r\n",
        "if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\r\n",
        "  os.makedirs(('/content/drive/My Drive/FYP/Existing'))\r\n",
        "c=None\r\n",
        "rt=None\r\n",
        "algo_name='WOA'\r\n",
        "if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\r\n",
        "  os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\r\n",
        "\r\n",
        "hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\r\n",
        "if not os.path.exists(('%s/%s')%(hp,ds_name)):\r\n",
        "    reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "    results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "    temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "    results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "    os.makedirs(('%s/%s')%(hp,ds_name))\r\n",
        "    os.makedirs(reduced_datasets)\r\n",
        "    os.makedirs(results_csv)\r\n",
        "    os.makedirs(results_graphs)\r\n",
        "    os.makedirs(temp_storage)\r\n",
        "  \r\n",
        "else:\r\n",
        "  print(\"Error Already File Exists!!!!\")\r\n",
        "  reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "  results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "  temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "  results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "  import sys\r\n",
        "  #sys.exit(\"Error message\")\r\n",
        "## Whale Optimization Algorithm (WOA)\r\n",
        "\r\n",
        "#from FS.woa import jfs \r\n",
        "k    = 5     # k-value in KNN\r\n",
        "N    = 100    # number of chromosomes\r\n",
        "T    = 31   # maximum number of generations\r\n",
        "b  = 1    # constant\r\n",
        "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'b':b}\r\n",
        "# perform feature selection\r\n",
        "WOA(feat, label, opts)\r\n",
        "# plot_pdf('naive bayes')\r\n",
        "# plot_pdf('tree')\r\n",
        "# plot_pdf('svm_linear')\r\n",
        "# plot_pdf('knn')\r\n",
        "\r\n",
        "# print(\"For Naive Bayes\")\r\n",
        "# mean_clf('naive bayes')\r\n",
        "# print(\"\\nFor Tree\")\r\n",
        "# mean_clf('tree')\r\n",
        "# print(\"\\nFor SVM Linear\")\r\n",
        "# mean_clf('svm_linear')\r\n",
        "# print(\"\\nFor KNN\")\r\n",
        "# mean_clf('knn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvoNBl33V_tF"
      },
      "source": [
        "# BA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMa9vT05V-jG"
      },
      "source": [
        "\r\n",
        "## creating directories\r\n",
        "\r\n",
        "if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\r\n",
        "  os.makedirs(('/content/drive/My Drive/FYP/Existing'))\r\n",
        "c=None\r\n",
        "rt=None\r\n",
        "algo_name='BA'\r\n",
        "if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\r\n",
        "  os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\r\n",
        "\r\n",
        "hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\r\n",
        "if not os.path.exists(('%s/%s')%(hp,ds_name)):\r\n",
        "    reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "    results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "    temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "    results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "    os.makedirs(('%s/%s')%(hp,ds_name))\r\n",
        "    os.makedirs(reduced_datasets)\r\n",
        "    os.makedirs(results_csv)\r\n",
        "    os.makedirs(results_graphs)\r\n",
        "    os.makedirs(temp_storage)\r\n",
        "  \r\n",
        "else:\r\n",
        "  print(\"Error Already File Exists!!!!\")\r\n",
        "  reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "  results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "  temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "  results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "  import sys\r\n",
        "  #sys.exit(\"Error message\")\r\n",
        "## Bat Algorithm (BA)\r\n",
        "#from FS.ba import jfs \r\n",
        "k    = 5     # k-value in KNN\r\n",
        "N    = 100    # number of chromosomes\r\n",
        "T    = 31   # maximum number of generations\r\n",
        "fmax   = 2      # maximum frequency\r\n",
        "fmin   = 0      # minimum frequency\r\n",
        "alpha  = 0.9    # constant\r\n",
        "gamma  = 0.9    # constant\r\n",
        "A      = 2      # maximum loudness\r\n",
        "r      = 1      # maximum pulse rate\r\n",
        "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'fmax':fmax, 'fmin':fmin, 'alpha':alpha, 'gamma':gamma, 'A':A, 'r':r}\r\n",
        "# perform feature selection\r\n",
        "BA(feat, label, opts)\r\n",
        "# plot_pdf('naive bayes')\r\n",
        "# plot_pdf('tree')\r\n",
        "# plot_pdf('svm_linear')\r\n",
        "# plot_pdf('knn')\r\n",
        "\r\n",
        "# print(\"For Naive Bayes\")\r\n",
        "# mean_clf('naive bayes')\r\n",
        "# print(\"\\nFor Tree\")\r\n",
        "# mean_clf('tree')\r\n",
        "# print(\"\\nFor SVM Linear\")\r\n",
        "# mean_clf('svm_linear')\r\n",
        "# print(\"\\nFor KNN\")\r\n",
        "# mean_clf('knn')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_R9kzgWaxx"
      },
      "source": [
        "# HHO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMM-o21XWZid"
      },
      "source": [
        "\r\n",
        "\r\n",
        "## creating directories\r\n",
        "\r\n",
        "if not os.path.exists(('/content/drive/My Drive/FYP/Existing')):\r\n",
        "  os.makedirs(('/content/drive/My Drive/FYP/Existing'))\r\n",
        "c=None\r\n",
        "rt=None\r\n",
        "algo_name='HHO'\r\n",
        "if not os.path.exists((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name))):\r\n",
        "  os.makedirs((('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)))\r\n",
        "\r\n",
        "hp=('/content/drive/My Drive/FYP/Existing/%s')%(algo_name)\r\n",
        "if not os.path.exists(('%s/%s')%(hp,ds_name)):\r\n",
        "    reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "    results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "    temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "    results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "    os.makedirs(('%s/%s')%(hp,ds_name))\r\n",
        "    os.makedirs(reduced_datasets)\r\n",
        "    os.makedirs(results_csv)\r\n",
        "    os.makedirs(results_graphs)\r\n",
        "    os.makedirs(temp_storage)\r\n",
        "  \r\n",
        "else:\r\n",
        "  print(\"Error Already File Exists!!!!\")\r\n",
        "  reduced_datasets=('%s/%s/reduced_datasets')%(hp,ds_name)\r\n",
        "  results_csv=(('%s/%s/results_csv')%(hp,ds_name))\r\n",
        "  temp_storage=('%s/%s/temp')%(hp,ds_name)\r\n",
        "  results_graphs=(('%s/%s/results_graphs')%(hp,ds_name))\r\n",
        "  import sys\r\n",
        "  #sys.exit(\"Error message\")\r\n",
        "## Harris Hawk Optimization\t\r\n",
        "\r\n",
        "#from FS.hho import jfs \r\n",
        "k    = 5     # k-value in KNN\r\n",
        "N    = 100    # number of chromosomes\r\n",
        "T    = 30   # maximum number of generations\r\n",
        "opts = {'k':k, 'fold':fold, 'N':N, 'T':T}\r\n",
        "# perform feature selection\r\n",
        "HHO(feat, label, opts)\r\n",
        "# plot_pdf('naive bayes')\r\n",
        "# plot_pdf('tree')\r\n",
        "# plot_pdf('svm_linear')\r\n",
        "# plot_pdf('knn')\r\n",
        "\r\n",
        "# print(\"For Naive Bayes\")\r\n",
        "# mean_clf('naive bayes')\r\n",
        "# print(\"\\nFor Tree\")\r\n",
        "# mean_clf('tree')\r\n",
        "# print(\"\\nFor SVM Linear\")\r\n",
        "# mean_clf('svm_linear')\r\n",
        "# print(\"\\nFor KNN\")\r\n",
        "# mean_clf('knn')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}